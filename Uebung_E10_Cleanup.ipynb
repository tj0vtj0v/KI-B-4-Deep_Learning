{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "Deep Learning - KI29  \n",
    "Deggendorf Institute of Technology  \n",
    "Prof. Dr. Florian Wahl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Modell Klasse einfÃ¼hren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Initialization Code\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initilalize weights and biases according to the shape given\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Save Regularization Lambdas\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output as we did on the slides\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Regularisation\n",
    "        # L1 weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate the output based on inputs.\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Copy before we modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Set to 0 if value is <=0\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the per sample loss\n",
    "        samples_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate the mean loss and return it\n",
    "        loss = np.mean(samples_losses)\n",
    "        return loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # Init return value to 0\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "            \n",
    "        # L1 biases\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "        # L2 biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)  # Count the samples\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Clip the predictions\n",
    "\n",
    "        # Get correct confidence values\n",
    "        # if labels are sparse\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "\n",
    "        # else if labels are one hot encoded\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Compute Losses\n",
    "        losses = -np.log(correct_confidences)\n",
    "        return losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):  # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            # If layer has no momentum arrays, create them\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Compute weight updates\n",
    "            weight_updates = (\n",
    "                self.momentum * layer.weight_momentums\n",
    "                - self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Compute bias updates\n",
    "            bias_updates = (\n",
    "                self.momentum * layer.bias_momentums\n",
    "                - self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Perform update\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        )\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.001, decay=0.0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Compute momentums\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Perform momentum correction using beta 1 (add +1 to avoid zero-division error)\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        )\n",
    "        \n",
    "        # Perform cache correction\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * weight_momentums_corrected\n",
    "            / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * bias_momentums_corrected\n",
    "            / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        # Store the dropout probability which is 1-dropout_rate\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Save inputs\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Apply output mask\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Apply gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Initialization Code\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initilalize weights and biases according to the shape given\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Save Regularization Lambdas\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output as we did on the slides\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Regularisation\n",
    "        # L1 weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return samples_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Input:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Model Class Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self, *, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def finalize(self):\n",
    "        # Create the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count layers\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        # Iterate through and connect layers\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            elif i < layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                \n",
    "    def forward(self, X):\n",
    "        # Perform forward pass\n",
    "        self.input_layer.forward(X)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output)\n",
    "\n",
    "        return layer.output\n",
    "        \n",
    "    def train(self, X, y, *, epochs=1, print_every=1):\n",
    "        for epoch in range(1, epochs+1):\n",
    "            output = self.forward(X)\n",
    "            print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00]\n",
      " [-1.13209162e-05]\n",
      " [-2.26418324e-05]\n",
      " [-3.39627550e-05]\n",
      " [-4.52836648e-05]\n",
      " [-5.66045856e-05]\n",
      " [-6.79255099e-05]\n",
      " [-7.92464198e-05]\n",
      " [-9.05673296e-05]\n",
      " [-1.01888261e-04]\n",
      " [-1.13209171e-04]\n",
      " [-1.24530066e-04]\n",
      " [-1.35851020e-04]\n",
      " [-1.47171915e-04]\n",
      " [-1.58492840e-04]\n",
      " [-1.69813764e-04]\n",
      " [-1.81134659e-04]\n",
      " [-1.92455554e-04]\n",
      " [-2.03776523e-04]\n",
      " [-2.15097418e-04]\n",
      " [-2.26418342e-04]\n",
      " [-2.37739223e-04]\n",
      " [-2.49060133e-04]\n",
      " [-2.60381144e-04]\n",
      " [-2.71702040e-04]\n",
      " [-2.83022935e-04]\n",
      " [-2.94343830e-04]\n",
      " [-3.05664696e-04]\n",
      " [-3.16985679e-04]\n",
      " [-3.28306574e-04]\n",
      " [-3.39627528e-04]\n",
      " [-3.50948452e-04]\n",
      " [-3.62269318e-04]\n",
      " [-3.73590243e-04]\n",
      " [-3.84911109e-04]\n",
      " [-3.96232092e-04]\n",
      " [-4.07553045e-04]\n",
      " [-4.18873911e-04]\n",
      " [-4.30194836e-04]\n",
      " [-4.41515760e-04]\n",
      " [-4.52836684e-04]\n",
      " [-4.64157580e-04]\n",
      " [-4.75478446e-04]\n",
      " [-4.86799399e-04]\n",
      " [-4.98120266e-04]\n",
      " [-5.09441248e-04]\n",
      " [-5.20762289e-04]\n",
      " [-5.32083155e-04]\n",
      " [-5.43404080e-04]\n",
      " [-5.54724946e-04]\n",
      " [-5.66045870e-04]\n",
      " [-5.77366678e-04]\n",
      " [-5.88687661e-04]\n",
      " [-6.00008527e-04]\n",
      " [-6.11329393e-04]\n",
      " [-6.22650492e-04]\n",
      " [-6.33971358e-04]\n",
      " [-6.45292224e-04]\n",
      " [-6.56613149e-04]\n",
      " [-6.67934073e-04]\n",
      " [-6.79255056e-04]\n",
      " [-6.90575922e-04]\n",
      " [-7.01896905e-04]\n",
      " [-7.13217771e-04]\n",
      " [-7.24538637e-04]\n",
      " [-7.35859619e-04]\n",
      " [-7.47180486e-04]\n",
      " [-7.58501352e-04]\n",
      " [-7.69822218e-04]\n",
      " [-7.81143142e-04]\n",
      " [-7.92464183e-04]\n",
      " [-8.03785049e-04]\n",
      " [-8.15106090e-04]\n",
      " [-8.26426898e-04]\n",
      " [-8.37747823e-04]\n",
      " [-8.49068747e-04]\n",
      " [-8.60389671e-04]\n",
      " [-8.71710654e-04]\n",
      " [-8.83031520e-04]\n",
      " [-8.94352444e-04]\n",
      " [-9.05673369e-04]\n",
      " [-9.16994235e-04]\n",
      " [-9.28315159e-04]\n",
      " [-9.39636084e-04]\n",
      " [-9.50956892e-04]\n",
      " [-9.62277991e-04]\n",
      " [-9.73598799e-04]\n",
      " [-9.84919723e-04]\n",
      " [-9.96240531e-04]\n",
      " [-1.00756157e-03]\n",
      " [-1.01888250e-03]\n",
      " [-1.03020354e-03]\n",
      " [-1.04152458e-03]\n",
      " [-1.05284527e-03]\n",
      " [-1.06416631e-03]\n",
      " [-1.07548700e-03]\n",
      " [-1.08680816e-03]\n",
      " [-1.09812885e-03]\n",
      " [-1.10944989e-03]\n",
      " [-1.12077058e-03]\n",
      " [-1.13209174e-03]\n",
      " [-1.14341243e-03]\n",
      " [-1.15473336e-03]\n",
      " [-1.16605428e-03]\n",
      " [-1.17737532e-03]\n",
      " [-1.18869601e-03]\n",
      " [-1.20001705e-03]\n",
      " [-1.21133809e-03]\n",
      " [-1.22265879e-03]\n",
      " [-1.23397983e-03]\n",
      " [-1.24530098e-03]\n",
      " [-1.25662179e-03]\n",
      " [-1.26794272e-03]\n",
      " [-1.27926364e-03]\n",
      " [-1.29058445e-03]\n",
      " [-1.30190549e-03]\n",
      " [-1.31322630e-03]\n",
      " [-1.32454745e-03]\n",
      " [-1.33586815e-03]\n",
      " [-1.34718895e-03]\n",
      " [-1.35851011e-03]\n",
      " [-1.36983092e-03]\n",
      " [-1.38115184e-03]\n",
      " [-1.39247265e-03]\n",
      " [-1.40379381e-03]\n",
      " [-1.41511450e-03]\n",
      " [-1.42643554e-03]\n",
      " [-1.43775647e-03]\n",
      " [-1.44907727e-03]\n",
      " [-1.46039843e-03]\n",
      " [-1.47171924e-03]\n",
      " [-1.48303993e-03]\n",
      " [-1.49436097e-03]\n",
      " [-1.50568178e-03]\n",
      " [-1.51700270e-03]\n",
      " [-1.52832351e-03]\n",
      " [-1.53964444e-03]\n",
      " [-1.55096548e-03]\n",
      " [-1.56228628e-03]\n",
      " [-1.57360733e-03]\n",
      " [-1.58492837e-03]\n",
      " [-1.59624941e-03]\n",
      " [-1.60757010e-03]\n",
      " [-1.61889102e-03]\n",
      " [-1.63021218e-03]\n",
      " [-1.64153287e-03]\n",
      " [-1.65285380e-03]\n",
      " [-1.66417484e-03]\n",
      " [-1.67549565e-03]\n",
      " [-1.68681645e-03]\n",
      " [-1.69813749e-03]\n",
      " [-1.70945865e-03]\n",
      " [-1.72077934e-03]\n",
      " [-1.73210015e-03]\n",
      " [-1.74342131e-03]\n",
      " [-1.75474200e-03]\n",
      " [-1.76606304e-03]\n",
      " [-1.77738396e-03]\n",
      " [-1.78870489e-03]\n",
      " [-1.80002581e-03]\n",
      " [-1.81134674e-03]\n",
      " [-1.82266755e-03]\n",
      " [-1.83398847e-03]\n",
      " [-1.84530928e-03]\n",
      " [-1.85663032e-03]\n",
      " [-1.86795124e-03]\n",
      " [-1.87927217e-03]\n",
      " [-1.89059298e-03]\n",
      " [-1.90191378e-03]\n",
      " [-1.91323482e-03]\n",
      " [-1.92455598e-03]\n",
      " [-1.93587644e-03]\n",
      " [-1.94719760e-03]\n",
      " [-1.95851852e-03]\n",
      " [-1.96983945e-03]\n",
      " [-1.98116037e-03]\n",
      " [-1.99248106e-03]\n",
      " [-2.00380222e-03]\n",
      " [-2.01512314e-03]\n",
      " [-2.02644360e-03]\n",
      " [-2.03776499e-03]\n",
      " [-2.04908568e-03]\n",
      " [-2.06040707e-03]\n",
      " [-2.07172753e-03]\n",
      " [-2.08304916e-03]\n",
      " [-2.09436985e-03]\n",
      " [-2.10569054e-03]\n",
      " [-2.11701146e-03]\n",
      " [-2.12833262e-03]\n",
      " [-2.13965308e-03]\n",
      " [-2.15097400e-03]\n",
      " [-2.16229516e-03]\n",
      " [-2.17361632e-03]\n",
      " [-2.18493678e-03]\n",
      " [-2.19625770e-03]\n",
      " [-2.20757886e-03]\n",
      " [-2.21889978e-03]\n",
      " [-2.23022071e-03]\n",
      " [-2.24154117e-03]\n",
      " [-2.25286232e-03]\n",
      " [-2.26418348e-03]\n",
      " [-2.27550417e-03]\n",
      " [-2.28682486e-03]\n",
      " [-2.29814602e-03]\n",
      " [-2.30946671e-03]\n",
      " [-2.32078834e-03]\n",
      " [-2.33210856e-03]\n",
      " [-2.34342995e-03]\n",
      " [-2.35475064e-03]\n",
      " [-2.36607157e-03]\n",
      " [-2.37739203e-03]\n",
      " [-2.38871342e-03]\n",
      " [-2.40003411e-03]\n",
      " [-2.41135526e-03]\n",
      " [-2.42267619e-03]\n",
      " [-2.43399688e-03]\n",
      " [-2.44531757e-03]\n",
      " [-2.45663873e-03]\n",
      " [-2.46795965e-03]\n",
      " [-2.47928035e-03]\n",
      " [-2.49060197e-03]\n",
      " [-2.50192266e-03]\n",
      " [-2.51324358e-03]\n",
      " [-2.52456451e-03]\n",
      " [-2.53588543e-03]\n",
      " [-2.54720636e-03]\n",
      " [-2.55852728e-03]\n",
      " [-2.56984797e-03]\n",
      " [-2.58116890e-03]\n",
      " [-2.59249005e-03]\n",
      " [-2.60381098e-03]\n",
      " [-2.61513167e-03]\n",
      " [-2.62645259e-03]\n",
      " [-2.63777352e-03]\n",
      " [-2.64909491e-03]\n",
      " [-2.66041537e-03]\n",
      " [-2.67173629e-03]\n",
      " [-2.68305745e-03]\n",
      " [-2.69437791e-03]\n",
      " [-2.70569907e-03]\n",
      " [-2.71702022e-03]\n",
      " [-2.72834045e-03]\n",
      " [-2.73966184e-03]\n",
      " [-2.75098253e-03]\n",
      " [-2.76230369e-03]\n",
      " [-2.77362484e-03]\n",
      " [-2.78494530e-03]\n",
      " [-2.79626646e-03]\n",
      " [-2.80758762e-03]\n",
      " [-2.81890854e-03]\n",
      " [-2.83022900e-03]\n",
      " [-2.84155016e-03]\n",
      " [-2.85287108e-03]\n",
      " [-2.86419224e-03]\n",
      " [-2.87551293e-03]\n",
      " [-2.88683409e-03]\n",
      " [-2.89815455e-03]\n",
      " [-2.90947570e-03]\n",
      " [-2.92079686e-03]\n",
      " [-2.93211709e-03]\n",
      " [-2.94343848e-03]\n",
      " [-2.95475940e-03]\n",
      " [-2.96607986e-03]\n",
      " [-2.97740102e-03]\n",
      " [-2.98872194e-03]\n",
      " [-3.00004310e-03]\n",
      " [-3.01136356e-03]\n",
      " [-3.02268518e-03]\n",
      " [-3.03400541e-03]\n",
      " [-3.04532680e-03]\n",
      " [-3.05664702e-03]\n",
      " [-3.06796865e-03]\n",
      " [-3.07928887e-03]\n",
      " [-3.09061049e-03]\n",
      " [-3.10193095e-03]\n",
      " [-3.11325234e-03]\n",
      " [-3.12457257e-03]\n",
      " [-3.13589396e-03]\n",
      " [-3.14721465e-03]\n",
      " [-3.15853581e-03]\n",
      " [-3.16985673e-03]\n",
      " [-3.18117766e-03]\n",
      " [-3.19249881e-03]\n",
      " [-3.20381904e-03]\n",
      " [-3.21514020e-03]\n",
      " [-3.22646135e-03]\n",
      " [-3.23778205e-03]\n",
      " [-3.24910320e-03]\n",
      " [-3.26042436e-03]\n",
      " [-3.27174482e-03]\n",
      " [-3.28306574e-03]\n",
      " [-3.29438667e-03]\n",
      " [-3.30570759e-03]\n",
      " [-3.31702828e-03]\n",
      " [-3.32834967e-03]\n",
      " [-3.33967060e-03]\n",
      " [-3.35099129e-03]\n",
      " [-3.36231245e-03]\n",
      " [-3.37363291e-03]\n",
      " [-3.38495383e-03]\n",
      " [-3.39627499e-03]\n",
      " [-3.40759614e-03]\n",
      " [-3.41891730e-03]\n",
      " [-3.43023823e-03]\n",
      " [-3.44155869e-03]\n",
      " [-3.45287938e-03]\n",
      " [-3.46420030e-03]\n",
      " [-3.47552099e-03]\n",
      " [-3.48684262e-03]\n",
      " [-3.49816354e-03]\n",
      " [-3.50948400e-03]\n",
      " [-3.52080469e-03]\n",
      " [-3.53212608e-03]\n",
      " [-3.54344700e-03]\n",
      " [-3.55476793e-03]\n",
      " [-3.56608909e-03]\n",
      " [-3.57740978e-03]\n",
      " [-3.58873047e-03]\n",
      " [-3.60005163e-03]\n",
      " [-3.61137255e-03]\n",
      " [-3.62269348e-03]\n",
      " [-3.63401417e-03]\n",
      " [-3.64533509e-03]\n",
      " [-3.65665648e-03]\n",
      " [-3.66797694e-03]\n",
      " [-3.67929810e-03]\n",
      " [-3.69061856e-03]\n",
      " [-3.70193948e-03]\n",
      " [-3.71326064e-03]\n",
      " [-3.72458110e-03]\n",
      " [-3.73590249e-03]\n",
      " [-3.74722341e-03]\n",
      " [-3.75854434e-03]\n",
      " [-3.76986503e-03]\n",
      " [-3.78118595e-03]\n",
      " [-3.79250711e-03]\n",
      " [-3.80382757e-03]\n",
      " [-3.81514896e-03]\n",
      " [-3.82646965e-03]\n",
      " [-3.83779081e-03]\n",
      " [-3.84911196e-03]\n",
      " [-3.86043219e-03]\n",
      " [-3.87175288e-03]\n",
      " [-3.88307404e-03]\n",
      " [-3.89439519e-03]\n",
      " [-3.90571612e-03]\n",
      " [-3.91703704e-03]\n",
      " [-3.92835913e-03]\n",
      " [-3.93967889e-03]\n",
      " [-3.95099958e-03]\n",
      " [-3.96232074e-03]\n",
      " [-3.97364190e-03]\n",
      " [-3.98496212e-03]\n",
      " [-3.99628421e-03]\n",
      " [-4.00760444e-03]\n",
      " [-4.01892606e-03]\n",
      " [-4.03024629e-03]\n",
      " [-4.04156698e-03]\n",
      " [-4.05288721e-03]\n",
      " [-4.06420929e-03]\n",
      " [-4.07552999e-03]\n",
      " [-4.08685114e-03]\n",
      " [-4.09817137e-03]\n",
      " [-4.10949299e-03]\n",
      " [-4.12081415e-03]\n",
      " [-4.13213437e-03]\n",
      " [-4.14345507e-03]\n",
      " [-4.15477622e-03]\n",
      " [-4.16609831e-03]\n",
      " [-4.17741854e-03]\n",
      " [-4.18873969e-03]\n",
      " [-4.20006085e-03]\n",
      " [-4.21138108e-03]\n",
      " [-4.22270130e-03]\n",
      " [-4.23402293e-03]\n",
      " [-4.24534362e-03]\n",
      " [-4.25666524e-03]\n",
      " [-4.26798500e-03]\n",
      " [-4.27930616e-03]\n",
      " [-4.29062732e-03]\n",
      " [-4.30194801e-03]\n",
      " [-4.31326916e-03]\n",
      " [-4.32459032e-03]\n",
      " [-4.33591101e-03]\n",
      " [-4.34723264e-03]\n",
      " [-4.35855379e-03]\n",
      " [-4.36987355e-03]\n",
      " [-4.38119471e-03]\n",
      " [-4.39251540e-03]\n",
      " [-4.40383656e-03]\n",
      " [-4.41515772e-03]\n",
      " [-4.42647887e-03]\n",
      " [-4.43779957e-03]\n",
      " [-4.44912026e-03]\n",
      " [-4.46044141e-03]\n",
      " [-4.47176211e-03]\n",
      " [-4.48308233e-03]\n",
      " [-4.49440349e-03]\n",
      " [-4.50572465e-03]\n",
      " [-4.51704627e-03]\n",
      " [-4.52836696e-03]\n",
      " [-4.53968672e-03]\n",
      " [-4.55100834e-03]\n",
      " [-4.56232904e-03]\n",
      " [-4.57364973e-03]\n",
      " [-4.58497135e-03]\n",
      " [-4.59629204e-03]\n",
      " [-4.60761273e-03]\n",
      " [-4.61893342e-03]\n",
      " [-4.63025458e-03]\n",
      " [-4.64157667e-03]\n",
      " [-4.65289643e-03]\n",
      " [-4.66421712e-03]\n",
      " [-4.67553874e-03]\n",
      " [-4.68685990e-03]\n",
      " [-4.69818059e-03]\n",
      " [-4.70950129e-03]\n",
      " [-4.72082198e-03]\n",
      " [-4.73214313e-03]\n",
      " [-4.74346429e-03]\n",
      " [-4.75478405e-03]\n",
      " [-4.76610614e-03]\n",
      " [-4.77742683e-03]\n",
      " [-4.78874706e-03]\n",
      " [-4.80006821e-03]\n",
      " [-4.81139030e-03]\n",
      " [-4.82271053e-03]\n",
      " [-4.83403075e-03]\n",
      " [-4.84535238e-03]\n",
      " [-4.85667353e-03]\n",
      " [-4.86799376e-03]\n",
      " [-4.87931445e-03]\n",
      " [-4.89063514e-03]\n",
      " [-4.90195723e-03]\n",
      " [-4.91327746e-03]\n",
      " [-4.92459862e-03]\n",
      " [-4.93591931e-03]\n",
      " [-4.94724046e-03]\n",
      " [-4.95856069e-03]\n",
      " [-4.96988231e-03]\n",
      " [-4.98120394e-03]\n",
      " [-4.99252416e-03]\n",
      " [-5.00384532e-03]\n",
      " [-5.01516508e-03]\n",
      " [-5.02648717e-03]\n",
      " [-5.03780786e-03]\n",
      " [-5.04912902e-03]\n",
      " [-5.06045017e-03]\n",
      " [-5.07177087e-03]\n",
      " [-5.08309156e-03]\n",
      " [-5.09441271e-03]\n",
      " [-5.10573387e-03]\n",
      " [-5.11705456e-03]\n",
      " [-5.12837619e-03]\n",
      " [-5.13969595e-03]\n",
      " [-5.15101757e-03]\n",
      " [-5.16233779e-03]\n",
      " [-5.17365802e-03]\n",
      " [-5.18498011e-03]\n",
      " [-5.19630034e-03]\n",
      " [-5.20762196e-03]\n",
      " [-5.21894265e-03]\n",
      " [-5.23026334e-03]\n",
      " [-5.24158403e-03]\n",
      " [-5.25290519e-03]\n",
      " [-5.26422635e-03]\n",
      " [-5.27554704e-03]\n",
      " [-5.28686820e-03]\n",
      " [-5.29818982e-03]\n",
      " [-5.30950958e-03]\n",
      " [-5.32083074e-03]\n",
      " [-5.33215236e-03]\n",
      " [-5.34347259e-03]\n",
      " [-5.35479467e-03]\n",
      " [-5.36611490e-03]\n",
      " [-5.37743652e-03]\n",
      " [-5.38875582e-03]\n",
      " [-5.40007744e-03]\n",
      " [-5.41139813e-03]\n",
      " [-5.42271929e-03]\n",
      " [-5.43404045e-03]\n",
      " [-5.44536114e-03]\n",
      " [-5.45668090e-03]\n",
      " [-5.46800345e-03]\n",
      " [-5.47932368e-03]\n",
      " [-5.49064483e-03]\n",
      " [-5.50196506e-03]\n",
      " [-5.51328668e-03]\n",
      " [-5.52460738e-03]\n",
      " [-5.53592807e-03]\n",
      " [-5.54724969e-03]\n",
      " [-5.55856992e-03]\n",
      " [-5.56989061e-03]\n",
      " [-5.58121223e-03]\n",
      " [-5.59253292e-03]\n",
      " [-5.60385361e-03]\n",
      " [-5.61517524e-03]\n",
      " [-5.62649500e-03]\n",
      " [-5.63781708e-03]\n",
      " [-5.64913731e-03]\n",
      " [-5.66045800e-03]\n",
      " [-5.67177869e-03]\n",
      " [-5.68310032e-03]\n",
      " [-5.69442054e-03]\n",
      " [-5.70574217e-03]\n",
      " [-5.71706332e-03]\n",
      " [-5.72838448e-03]\n",
      " [-5.73970471e-03]\n",
      " [-5.75102586e-03]\n",
      " [-5.76234609e-03]\n",
      " [-5.77366818e-03]\n",
      " [-5.78498840e-03]\n",
      " [-5.79630909e-03]\n",
      " [-5.80762979e-03]\n",
      " [-5.81895141e-03]\n",
      " [-5.83027164e-03]\n",
      " [-5.84159372e-03]\n",
      " [-5.85291395e-03]\n",
      " [-5.86423418e-03]\n",
      " [-5.87555626e-03]\n",
      " [-5.88687696e-03]\n",
      " [-5.89819672e-03]\n",
      " [-5.90951880e-03]\n",
      " [-5.92083996e-03]\n",
      " [-5.93215972e-03]\n",
      " [-5.94348041e-03]\n",
      " [-5.95480204e-03]\n",
      " [-5.96612273e-03]\n",
      " [-5.97744389e-03]\n",
      " [-5.98876504e-03]\n",
      " [-6.00008620e-03]\n",
      " [-6.01140689e-03]\n",
      " [-6.02272712e-03]\n",
      " [-6.03404921e-03]\n",
      " [-6.04537036e-03]\n",
      " [-6.05669105e-03]\n",
      " [-6.06801081e-03]\n",
      " [-6.07933244e-03]\n",
      " [-6.09065359e-03]\n",
      " [-6.10197382e-03]\n",
      " [-6.11329405e-03]\n",
      " [-6.12461567e-03]\n",
      " [-6.13593729e-03]\n",
      " [-6.14725752e-03]\n",
      " [-6.15857774e-03]\n",
      " [-6.16989983e-03]\n",
      " [-6.18122099e-03]\n",
      " [-6.19254075e-03]\n",
      " [-6.20386191e-03]\n",
      " [-6.21518306e-03]\n",
      " [-6.22650469e-03]\n",
      " [-6.23782584e-03]\n",
      " [-6.24914514e-03]\n",
      " [-6.26046676e-03]\n",
      " [-6.27178792e-03]\n",
      " [-6.28310908e-03]\n",
      " [-6.29442930e-03]\n",
      " [-6.30575093e-03]\n",
      " [-6.31707162e-03]\n",
      " [-6.32839231e-03]\n",
      " [-6.33971347e-03]\n",
      " [-6.35103509e-03]\n",
      " [-6.36235531e-03]\n",
      " [-6.37367601e-03]\n",
      " [-6.38499763e-03]\n",
      " [-6.39631739e-03]\n",
      " [-6.40763808e-03]\n",
      " [-6.41895924e-03]\n",
      " [-6.43028039e-03]\n",
      " [-6.44160295e-03]\n",
      " [-6.45292271e-03]\n",
      " [-6.46424340e-03]\n",
      " [-6.47556409e-03]\n",
      " [-6.48688572e-03]\n",
      " [-6.49820641e-03]\n",
      " [-6.50952663e-03]\n",
      " [-6.52084872e-03]\n",
      " [-6.53216895e-03]\n",
      " [-6.54348964e-03]\n",
      " [-6.55480986e-03]\n",
      " [-6.56613149e-03]\n",
      " [-6.57745125e-03]\n",
      " [-6.58877334e-03]\n",
      " [-6.60009403e-03]\n",
      " [-6.61141519e-03]\n",
      " [-6.62273681e-03]\n",
      " [-6.63405657e-03]\n",
      " [-6.64537819e-03]\n",
      " [-6.65669935e-03]\n",
      " [-6.66802051e-03]\n",
      " [-6.67934120e-03]\n",
      " [-6.69066142e-03]\n",
      " [-6.70198258e-03]\n",
      " [-6.71330327e-03]\n",
      " [-6.72462489e-03]\n",
      " [-6.73594559e-03]\n",
      " [-6.74726581e-03]\n",
      " [-6.75858650e-03]\n",
      " [-6.76990766e-03]\n",
      " [-6.78122928e-03]\n",
      " [-6.79254998e-03]\n",
      " [-6.80387067e-03]\n",
      " [-6.81519229e-03]\n",
      " [-6.82651298e-03]\n",
      " [-6.83783460e-03]\n",
      " [-6.84915483e-03]\n",
      " [-6.86047645e-03]\n",
      " [-6.87179575e-03]\n",
      " [-6.88311737e-03]\n",
      " [-6.89443806e-03]\n",
      " [-6.90575875e-03]\n",
      " [-6.91708038e-03]\n",
      " [-6.92840060e-03]\n",
      " [-6.93972176e-03]\n",
      " [-6.95104199e-03]\n",
      " [-6.96236361e-03]\n",
      " [-6.97368523e-03]\n",
      " [-6.98500639e-03]\n",
      " [-6.99632708e-03]\n",
      " [-7.00764824e-03]\n",
      " [-7.01896800e-03]\n",
      " [-7.03028962e-03]\n",
      " [-7.04160938e-03]\n",
      " [-7.05293054e-03]\n",
      " [-7.06425216e-03]\n",
      " [-7.07557192e-03]\n",
      " [-7.08689401e-03]\n",
      " [-7.09821424e-03]\n",
      " [-7.10953586e-03]\n",
      " [-7.12085608e-03]\n",
      " [-7.13217817e-03]\n",
      " [-7.14349886e-03]\n",
      " [-7.15481956e-03]\n",
      " [-7.16614118e-03]\n",
      " [-7.17746094e-03]\n",
      " [-7.18878163e-03]\n",
      " [-7.20010325e-03]\n",
      " [-7.21142394e-03]\n",
      " [-7.22274510e-03]\n",
      " [-7.23406533e-03]\n",
      " [-7.24538695e-03]\n",
      " [-7.25670671e-03]\n",
      " [-7.26802833e-03]\n",
      " [-7.27934856e-03]\n",
      " [-7.29067018e-03]\n",
      " [-7.30199181e-03]\n",
      " [-7.31331296e-03]\n",
      " [-7.32463459e-03]\n",
      " [-7.33595388e-03]\n",
      " [-7.34727504e-03]\n",
      " [-7.35859619e-03]\n",
      " [-7.36991689e-03]\n",
      " [-7.38123711e-03]\n",
      " [-7.39255734e-03]\n",
      " [-7.40387896e-03]\n",
      " [-7.41520012e-03]\n",
      " [-7.42652128e-03]\n",
      " [-7.43784197e-03]\n",
      " [-7.44916219e-03]\n",
      " [-7.46048288e-03]\n",
      " [-7.47180497e-03]\n",
      " [-7.48312660e-03]\n",
      " [-7.49444682e-03]\n",
      " [-7.50576798e-03]\n",
      " [-7.51708867e-03]\n",
      " [-7.52841029e-03]\n",
      " [-7.53973005e-03]\n",
      " [-7.55105121e-03]\n",
      " [-7.56237190e-03]\n",
      " [-7.57369399e-03]\n",
      " [-7.58501422e-03]\n",
      " [-7.59633537e-03]\n",
      " [-7.60765513e-03]\n",
      " [-7.61897629e-03]\n",
      " [-7.63029791e-03]\n",
      " [-7.64162000e-03]\n",
      " [-7.65293930e-03]\n",
      " [-7.66426092e-03]\n",
      " [-7.67558161e-03]\n",
      " [-7.68690277e-03]\n",
      " [-7.69822393e-03]\n",
      " [-7.70954462e-03]\n",
      " [-7.72086438e-03]\n",
      " [-7.73218600e-03]\n",
      " [-7.74350576e-03]\n",
      " [-7.75482832e-03]\n",
      " [-7.76614808e-03]\n",
      " [-7.77746877e-03]\n",
      " [-7.78879039e-03]\n",
      " [-7.80011294e-03]\n",
      " [-7.81143224e-03]\n",
      " [-7.82275386e-03]\n",
      " [-7.83407409e-03]\n",
      " [-7.84539431e-03]\n",
      " [-7.85671826e-03]\n",
      " [-7.86803756e-03]\n",
      " [-7.87935778e-03]\n",
      " [-7.89067894e-03]\n",
      " [-7.90199917e-03]\n",
      " [-7.91331939e-03]\n",
      " [-7.92464148e-03]\n",
      " [-7.93596264e-03]\n",
      " [-7.94728380e-03]\n",
      " [-7.95860495e-03]\n",
      " [-7.96992425e-03]\n",
      " [-7.98124820e-03]\n",
      " [-7.99256843e-03]\n",
      " [-8.00388772e-03]\n",
      " [-8.01520888e-03]\n",
      " [-8.02652910e-03]\n",
      " [-8.03785212e-03]\n",
      " [-8.04917235e-03]\n",
      " [-8.06049258e-03]\n",
      " [-8.07181373e-03]\n",
      " [-8.08313396e-03]\n",
      " [-8.09445605e-03]\n",
      " [-8.10577441e-03]\n",
      " [-8.11709650e-03]\n",
      " [-8.12841859e-03]\n",
      " [-8.13973974e-03]\n",
      " [-8.15105997e-03]\n",
      " [-8.16238113e-03]\n",
      " [-8.17370228e-03]\n",
      " [-8.18502251e-03]\n",
      " [-8.19634274e-03]\n",
      " [-8.20766389e-03]\n",
      " [-8.21898598e-03]\n",
      " [-8.23030714e-03]\n",
      " [-8.24162830e-03]\n",
      " [-8.25294666e-03]\n",
      " [-8.26426875e-03]\n",
      " [-8.27558991e-03]\n",
      " [-8.28691013e-03]\n",
      " [-8.29823408e-03]\n",
      " [-8.30955245e-03]\n",
      " [-8.32087640e-03]\n",
      " [-8.33219662e-03]\n",
      " [-8.34351499e-03]\n",
      " [-8.35483707e-03]\n",
      " [-8.36615637e-03]\n",
      " [-8.37747939e-03]\n",
      " [-8.38879868e-03]\n",
      " [-8.40012170e-03]\n",
      " [-8.41144193e-03]\n",
      " [-8.42276216e-03]\n",
      " [-8.43408238e-03]\n",
      " [-8.44540261e-03]\n",
      " [-8.45672376e-03]\n",
      " [-8.46804585e-03]\n",
      " [-8.47936701e-03]\n",
      " [-8.49068724e-03]\n",
      " [-8.50200839e-03]\n",
      " [-8.51333048e-03]\n",
      " [-8.52465164e-03]\n",
      " [-8.53597000e-03]\n",
      " [-8.54729116e-03]\n",
      " [-8.55861232e-03]\n",
      " [-8.56993347e-03]\n",
      " [-8.58125463e-03]\n",
      " [-8.59257579e-03]\n",
      " [-8.60389601e-03]\n",
      " [-8.61521717e-03]\n",
      " [-8.62653833e-03]\n",
      " [-8.63785949e-03]\n",
      " [-8.64918064e-03]\n",
      " [-8.66050087e-03]\n",
      " [-8.67182203e-03]\n",
      " [-8.68314411e-03]\n",
      " [-8.69446527e-03]\n",
      " [-8.70578550e-03]\n",
      " [-8.71710759e-03]\n",
      " [-8.72842502e-03]\n",
      " [-8.73974711e-03]\n",
      " [-8.75106826e-03]\n",
      " [-8.76238942e-03]\n",
      " [-8.77370965e-03]\n",
      " [-8.78503080e-03]\n",
      " [-8.79635196e-03]\n",
      " [-8.80767312e-03]\n",
      " [-8.81899334e-03]\n",
      " [-8.83031543e-03]\n",
      " [-8.84163659e-03]\n",
      " [-8.85295775e-03]\n",
      " [-8.86427891e-03]\n",
      " [-8.87559913e-03]\n",
      " [-8.88691936e-03]\n",
      " [-8.89824051e-03]\n",
      " [-8.90955981e-03]\n",
      " [-8.92088283e-03]\n",
      " [-8.93220305e-03]\n",
      " [-8.94352421e-03]\n",
      " [-8.95484537e-03]\n",
      " [-8.96616466e-03]\n",
      " [-8.97748861e-03]\n",
      " [-8.98880698e-03]\n",
      " [-9.00012814e-03]\n",
      " [-9.01144929e-03]\n",
      " [-9.02277138e-03]\n",
      " [-9.03409254e-03]\n",
      " [-9.04541276e-03]\n",
      " [-9.05673392e-03]\n",
      " [-9.06805415e-03]\n",
      " [-9.07937344e-03]\n",
      " [-9.09069553e-03]\n",
      " [-9.10201669e-03]\n",
      " [-9.11333784e-03]\n",
      " [-9.12465807e-03]\n",
      " [-9.13597923e-03]\n",
      " [-9.14729945e-03]\n",
      " [-9.15861968e-03]\n",
      " [-9.16994270e-03]\n",
      " [-9.18126386e-03]\n",
      " [-9.19258408e-03]\n",
      " [-9.20390524e-03]\n",
      " [-9.21522547e-03]\n",
      " [-9.22654849e-03]\n",
      " [-9.23786685e-03]\n",
      " [-9.24918801e-03]\n",
      " [-9.26050916e-03]\n",
      " [-9.27183218e-03]\n",
      " [-9.28315334e-03]\n",
      " [-9.29447357e-03]\n",
      " [-9.30579286e-03]\n",
      " [-9.31711402e-03]\n",
      " [-9.32843424e-03]\n",
      " [-9.33975540e-03]\n",
      " [-9.35107749e-03]\n",
      " [-9.36239772e-03]\n",
      " [-9.37371980e-03]\n",
      " [-9.38503910e-03]\n",
      " [-9.39636119e-03]\n",
      " [-9.40768141e-03]\n",
      " [-9.41900257e-03]\n",
      " [-9.43032373e-03]\n",
      " [-9.44164395e-03]\n",
      " [-9.45296511e-03]\n",
      " [-9.46428627e-03]\n",
      " [-9.47560836e-03]\n",
      " [-9.48692858e-03]\n",
      " [-9.49824788e-03]\n",
      " [-9.50956810e-03]\n",
      " [-9.52089205e-03]\n",
      " [-9.53221228e-03]\n",
      " [-9.54353251e-03]\n",
      " [-9.55485366e-03]\n",
      " [-9.56617482e-03]\n",
      " [-9.57749411e-03]\n",
      " [-9.58881620e-03]\n",
      " [-9.60013643e-03]\n",
      " [-9.61145852e-03]\n",
      " [-9.62278061e-03]\n",
      " [-9.63409990e-03]\n",
      " [-9.64542106e-03]\n",
      " [-9.65674128e-03]\n",
      " [-9.66806151e-03]\n",
      " [-9.67938360e-03]\n",
      " [-9.69070476e-03]\n",
      " [-9.70202684e-03]\n",
      " [-9.71334707e-03]\n",
      " [-9.72466730e-03]\n",
      " [-9.73598752e-03]\n",
      " [-9.74730868e-03]\n",
      " [-9.75862890e-03]\n",
      " [-9.76995192e-03]\n",
      " [-9.78127029e-03]\n",
      " [-9.79259238e-03]\n",
      " [-9.80391447e-03]\n",
      " [-9.81523469e-03]\n",
      " [-9.82655492e-03]\n",
      " [-9.83787607e-03]\n",
      " [-9.84919723e-03]\n",
      " [-9.86051746e-03]\n",
      " [-9.87183861e-03]\n",
      " [-9.88316163e-03]\n",
      " [-9.89448093e-03]\n",
      " [-9.90580115e-03]\n",
      " [-9.91712138e-03]\n",
      " [-9.92844440e-03]\n",
      " [-9.93976463e-03]\n",
      " [-9.95108671e-03]\n",
      " [-9.96240787e-03]\n",
      " [-9.97372903e-03]\n",
      " [-9.98504832e-03]\n",
      " [-9.99637134e-03]\n",
      " [-1.00076906e-02]\n",
      " [-1.00190109e-02]\n",
      " [-1.00303302e-02]\n",
      " [-1.00416522e-02]\n",
      " [-1.00529743e-02]\n",
      " [-1.00642955e-02]\n",
      " [-1.00756157e-02]\n",
      " [-1.00869369e-02]\n",
      " [-1.00982580e-02]\n",
      " [-1.01095792e-02]\n",
      " [-1.01209003e-02]\n",
      " [-1.01322206e-02]\n",
      " [-1.01435417e-02]\n",
      " [-1.01548620e-02]\n",
      " [-1.01661831e-02]\n",
      " [-1.01775033e-02]\n",
      " [-1.01888254e-02]\n",
      " [-1.02001457e-02]\n",
      " [-1.02114677e-02]\n",
      " [-1.02227880e-02]\n",
      " [-1.02341091e-02]\n",
      " [-1.02454294e-02]\n",
      " [-1.02567524e-02]\n",
      " [-1.02680717e-02]\n",
      " [-1.02793919e-02]\n",
      " [-1.02907130e-02]\n",
      " [-1.03020351e-02]\n",
      " [-1.03133563e-02]\n",
      " [-1.03246756e-02]\n",
      " [-1.03359958e-02]\n",
      " [-1.03473160e-02]\n",
      " [-1.03586391e-02]\n",
      " [-1.03699602e-02]\n",
      " [-1.03812814e-02]\n",
      " [-1.03926007e-02]\n",
      " [-1.04039228e-02]\n",
      " [-1.04152439e-02]\n",
      " [-1.04265651e-02]\n",
      " [-1.04378853e-02]\n",
      " [-1.04492074e-02]\n",
      " [-1.04605267e-02]\n",
      " [-1.04718478e-02]\n",
      " [-1.04831681e-02]\n",
      " [-1.04944883e-02]\n",
      " [-1.05058104e-02]\n",
      " [-1.05171315e-02]\n",
      " [-1.05284527e-02]\n",
      " [-1.05397720e-02]\n",
      " [-1.05510941e-02]\n",
      " [-1.05624143e-02]\n",
      " [-1.05737364e-02]\n",
      " [-1.05850585e-02]\n",
      " [-1.05963796e-02]\n",
      " [-1.06077008e-02]\n",
      " [-1.06190192e-02]\n",
      " [-1.06303412e-02]\n",
      " [-1.06416615e-02]\n",
      " [-1.06529836e-02]\n",
      " [-1.06643047e-02]\n",
      " [-1.06756240e-02]\n",
      " [-1.06869452e-02]\n",
      " [-1.06982654e-02]\n",
      " [-1.07095893e-02]\n",
      " [-1.07209077e-02]\n",
      " [-1.07322298e-02]\n",
      " [-1.07435491e-02]\n",
      " [-1.07548730e-02]\n",
      " [-1.07661923e-02]\n",
      " [-1.07775116e-02]\n",
      " [-1.07888337e-02]\n",
      " [-1.08001549e-02]\n",
      " [-1.08114760e-02]\n",
      " [-1.08227963e-02]\n",
      " [-1.08341156e-02]\n",
      " [-1.08454386e-02]\n",
      " [-1.08567579e-02]\n",
      " [-1.08680809e-02]\n",
      " [-1.08794002e-02]\n",
      " [-1.08907223e-02]\n",
      " [-1.09020444e-02]\n",
      " [-1.09133618e-02]\n",
      " [-1.09246839e-02]\n",
      " [-1.09360069e-02]\n",
      " [-1.09473262e-02]\n",
      " [-1.09586474e-02]\n",
      " [-1.09699667e-02]\n",
      " [-1.09812897e-02]\n",
      " [-1.09926099e-02]\n",
      " [-1.10039301e-02]\n",
      " [-1.10152531e-02]\n",
      " [-1.10265734e-02]\n",
      " [-1.10378927e-02]\n",
      " [-1.10492148e-02]\n",
      " [-1.10605350e-02]\n",
      " [-1.10718561e-02]\n",
      " [-1.10831782e-02]\n",
      " [-1.10944994e-02]\n",
      " [-1.11058205e-02]\n",
      " [-1.11171398e-02]\n",
      " [-1.11284619e-02]\n",
      " [-1.11397812e-02]\n",
      " [-1.11511033e-02]\n",
      " [-1.11624245e-02]\n",
      " [-1.11737447e-02]\n",
      " [-1.11850658e-02]\n",
      " [-1.11963851e-02]\n",
      " [-1.12077072e-02]\n",
      " [-1.12190275e-02]\n",
      " [-1.12303505e-02]\n",
      " [-1.12416707e-02]\n",
      " [-1.12529900e-02]\n",
      " [-1.12643130e-02]\n",
      " [-1.12756342e-02]\n",
      " [-1.12869553e-02]\n",
      " [-1.12982746e-02]\n",
      " [-1.13095958e-02]]\n",
      "[[ 0.00000000e+00]\n",
      " [-1.13209162e-05]\n",
      " [-2.26418324e-05]\n",
      " [-3.39627550e-05]\n",
      " [-4.52836648e-05]\n",
      " [-5.66045856e-05]\n",
      " [-6.79255099e-05]\n",
      " [-7.92464198e-05]\n",
      " [-9.05673296e-05]\n",
      " [-1.01888261e-04]\n",
      " [-1.13209171e-04]\n",
      " [-1.24530066e-04]\n",
      " [-1.35851020e-04]\n",
      " [-1.47171915e-04]\n",
      " [-1.58492840e-04]\n",
      " [-1.69813764e-04]\n",
      " [-1.81134659e-04]\n",
      " [-1.92455554e-04]\n",
      " [-2.03776523e-04]\n",
      " [-2.15097418e-04]\n",
      " [-2.26418342e-04]\n",
      " [-2.37739223e-04]\n",
      " [-2.49060133e-04]\n",
      " [-2.60381144e-04]\n",
      " [-2.71702040e-04]\n",
      " [-2.83022935e-04]\n",
      " [-2.94343830e-04]\n",
      " [-3.05664696e-04]\n",
      " [-3.16985679e-04]\n",
      " [-3.28306574e-04]\n",
      " [-3.39627528e-04]\n",
      " [-3.50948452e-04]\n",
      " [-3.62269318e-04]\n",
      " [-3.73590243e-04]\n",
      " [-3.84911109e-04]\n",
      " [-3.96232092e-04]\n",
      " [-4.07553045e-04]\n",
      " [-4.18873911e-04]\n",
      " [-4.30194836e-04]\n",
      " [-4.41515760e-04]\n",
      " [-4.52836684e-04]\n",
      " [-4.64157580e-04]\n",
      " [-4.75478446e-04]\n",
      " [-4.86799399e-04]\n",
      " [-4.98120266e-04]\n",
      " [-5.09441248e-04]\n",
      " [-5.20762289e-04]\n",
      " [-5.32083155e-04]\n",
      " [-5.43404080e-04]\n",
      " [-5.54724946e-04]\n",
      " [-5.66045870e-04]\n",
      " [-5.77366678e-04]\n",
      " [-5.88687661e-04]\n",
      " [-6.00008527e-04]\n",
      " [-6.11329393e-04]\n",
      " [-6.22650492e-04]\n",
      " [-6.33971358e-04]\n",
      " [-6.45292224e-04]\n",
      " [-6.56613149e-04]\n",
      " [-6.67934073e-04]\n",
      " [-6.79255056e-04]\n",
      " [-6.90575922e-04]\n",
      " [-7.01896905e-04]\n",
      " [-7.13217771e-04]\n",
      " [-7.24538637e-04]\n",
      " [-7.35859619e-04]\n",
      " [-7.47180486e-04]\n",
      " [-7.58501352e-04]\n",
      " [-7.69822218e-04]\n",
      " [-7.81143142e-04]\n",
      " [-7.92464183e-04]\n",
      " [-8.03785049e-04]\n",
      " [-8.15106090e-04]\n",
      " [-8.26426898e-04]\n",
      " [-8.37747823e-04]\n",
      " [-8.49068747e-04]\n",
      " [-8.60389671e-04]\n",
      " [-8.71710654e-04]\n",
      " [-8.83031520e-04]\n",
      " [-8.94352444e-04]\n",
      " [-9.05673369e-04]\n",
      " [-9.16994235e-04]\n",
      " [-9.28315159e-04]\n",
      " [-9.39636084e-04]\n",
      " [-9.50956892e-04]\n",
      " [-9.62277991e-04]\n",
      " [-9.73598799e-04]\n",
      " [-9.84919723e-04]\n",
      " [-9.96240531e-04]\n",
      " [-1.00756157e-03]\n",
      " [-1.01888250e-03]\n",
      " [-1.03020354e-03]\n",
      " [-1.04152458e-03]\n",
      " [-1.05284527e-03]\n",
      " [-1.06416631e-03]\n",
      " [-1.07548700e-03]\n",
      " [-1.08680816e-03]\n",
      " [-1.09812885e-03]\n",
      " [-1.10944989e-03]\n",
      " [-1.12077058e-03]\n",
      " [-1.13209174e-03]\n",
      " [-1.14341243e-03]\n",
      " [-1.15473336e-03]\n",
      " [-1.16605428e-03]\n",
      " [-1.17737532e-03]\n",
      " [-1.18869601e-03]\n",
      " [-1.20001705e-03]\n",
      " [-1.21133809e-03]\n",
      " [-1.22265879e-03]\n",
      " [-1.23397983e-03]\n",
      " [-1.24530098e-03]\n",
      " [-1.25662179e-03]\n",
      " [-1.26794272e-03]\n",
      " [-1.27926364e-03]\n",
      " [-1.29058445e-03]\n",
      " [-1.30190549e-03]\n",
      " [-1.31322630e-03]\n",
      " [-1.32454745e-03]\n",
      " [-1.33586815e-03]\n",
      " [-1.34718895e-03]\n",
      " [-1.35851011e-03]\n",
      " [-1.36983092e-03]\n",
      " [-1.38115184e-03]\n",
      " [-1.39247265e-03]\n",
      " [-1.40379381e-03]\n",
      " [-1.41511450e-03]\n",
      " [-1.42643554e-03]\n",
      " [-1.43775647e-03]\n",
      " [-1.44907727e-03]\n",
      " [-1.46039843e-03]\n",
      " [-1.47171924e-03]\n",
      " [-1.48303993e-03]\n",
      " [-1.49436097e-03]\n",
      " [-1.50568178e-03]\n",
      " [-1.51700270e-03]\n",
      " [-1.52832351e-03]\n",
      " [-1.53964444e-03]\n",
      " [-1.55096548e-03]\n",
      " [-1.56228628e-03]\n",
      " [-1.57360733e-03]\n",
      " [-1.58492837e-03]\n",
      " [-1.59624941e-03]\n",
      " [-1.60757010e-03]\n",
      " [-1.61889102e-03]\n",
      " [-1.63021218e-03]\n",
      " [-1.64153287e-03]\n",
      " [-1.65285380e-03]\n",
      " [-1.66417484e-03]\n",
      " [-1.67549565e-03]\n",
      " [-1.68681645e-03]\n",
      " [-1.69813749e-03]\n",
      " [-1.70945865e-03]\n",
      " [-1.72077934e-03]\n",
      " [-1.73210015e-03]\n",
      " [-1.74342131e-03]\n",
      " [-1.75474200e-03]\n",
      " [-1.76606304e-03]\n",
      " [-1.77738396e-03]\n",
      " [-1.78870489e-03]\n",
      " [-1.80002581e-03]\n",
      " [-1.81134674e-03]\n",
      " [-1.82266755e-03]\n",
      " [-1.83398847e-03]\n",
      " [-1.84530928e-03]\n",
      " [-1.85663032e-03]\n",
      " [-1.86795124e-03]\n",
      " [-1.87927217e-03]\n",
      " [-1.89059298e-03]\n",
      " [-1.90191378e-03]\n",
      " [-1.91323482e-03]\n",
      " [-1.92455598e-03]\n",
      " [-1.93587644e-03]\n",
      " [-1.94719760e-03]\n",
      " [-1.95851852e-03]\n",
      " [-1.96983945e-03]\n",
      " [-1.98116037e-03]\n",
      " [-1.99248106e-03]\n",
      " [-2.00380222e-03]\n",
      " [-2.01512314e-03]\n",
      " [-2.02644360e-03]\n",
      " [-2.03776499e-03]\n",
      " [-2.04908568e-03]\n",
      " [-2.06040707e-03]\n",
      " [-2.07172753e-03]\n",
      " [-2.08304916e-03]\n",
      " [-2.09436985e-03]\n",
      " [-2.10569054e-03]\n",
      " [-2.11701146e-03]\n",
      " [-2.12833262e-03]\n",
      " [-2.13965308e-03]\n",
      " [-2.15097400e-03]\n",
      " [-2.16229516e-03]\n",
      " [-2.17361632e-03]\n",
      " [-2.18493678e-03]\n",
      " [-2.19625770e-03]\n",
      " [-2.20757886e-03]\n",
      " [-2.21889978e-03]\n",
      " [-2.23022071e-03]\n",
      " [-2.24154117e-03]\n",
      " [-2.25286232e-03]\n",
      " [-2.26418348e-03]\n",
      " [-2.27550417e-03]\n",
      " [-2.28682486e-03]\n",
      " [-2.29814602e-03]\n",
      " [-2.30946671e-03]\n",
      " [-2.32078834e-03]\n",
      " [-2.33210856e-03]\n",
      " [-2.34342995e-03]\n",
      " [-2.35475064e-03]\n",
      " [-2.36607157e-03]\n",
      " [-2.37739203e-03]\n",
      " [-2.38871342e-03]\n",
      " [-2.40003411e-03]\n",
      " [-2.41135526e-03]\n",
      " [-2.42267619e-03]\n",
      " [-2.43399688e-03]\n",
      " [-2.44531757e-03]\n",
      " [-2.45663873e-03]\n",
      " [-2.46795965e-03]\n",
      " [-2.47928035e-03]\n",
      " [-2.49060197e-03]\n",
      " [-2.50192266e-03]\n",
      " [-2.51324358e-03]\n",
      " [-2.52456451e-03]\n",
      " [-2.53588543e-03]\n",
      " [-2.54720636e-03]\n",
      " [-2.55852728e-03]\n",
      " [-2.56984797e-03]\n",
      " [-2.58116890e-03]\n",
      " [-2.59249005e-03]\n",
      " [-2.60381098e-03]\n",
      " [-2.61513167e-03]\n",
      " [-2.62645259e-03]\n",
      " [-2.63777352e-03]\n",
      " [-2.64909491e-03]\n",
      " [-2.66041537e-03]\n",
      " [-2.67173629e-03]\n",
      " [-2.68305745e-03]\n",
      " [-2.69437791e-03]\n",
      " [-2.70569907e-03]\n",
      " [-2.71702022e-03]\n",
      " [-2.72834045e-03]\n",
      " [-2.73966184e-03]\n",
      " [-2.75098253e-03]\n",
      " [-2.76230369e-03]\n",
      " [-2.77362484e-03]\n",
      " [-2.78494530e-03]\n",
      " [-2.79626646e-03]\n",
      " [-2.80758762e-03]\n",
      " [-2.81890854e-03]\n",
      " [-2.83022900e-03]\n",
      " [-2.84155016e-03]\n",
      " [-2.85287108e-03]\n",
      " [-2.86419224e-03]\n",
      " [-2.87551293e-03]\n",
      " [-2.88683409e-03]\n",
      " [-2.89815455e-03]\n",
      " [-2.90947570e-03]\n",
      " [-2.92079686e-03]\n",
      " [-2.93211709e-03]\n",
      " [-2.94343848e-03]\n",
      " [-2.95475940e-03]\n",
      " [-2.96607986e-03]\n",
      " [-2.97740102e-03]\n",
      " [-2.98872194e-03]\n",
      " [-3.00004310e-03]\n",
      " [-3.01136356e-03]\n",
      " [-3.02268518e-03]\n",
      " [-3.03400541e-03]\n",
      " [-3.04532680e-03]\n",
      " [-3.05664702e-03]\n",
      " [-3.06796865e-03]\n",
      " [-3.07928887e-03]\n",
      " [-3.09061049e-03]\n",
      " [-3.10193095e-03]\n",
      " [-3.11325234e-03]\n",
      " [-3.12457257e-03]\n",
      " [-3.13589396e-03]\n",
      " [-3.14721465e-03]\n",
      " [-3.15853581e-03]\n",
      " [-3.16985673e-03]\n",
      " [-3.18117766e-03]\n",
      " [-3.19249881e-03]\n",
      " [-3.20381904e-03]\n",
      " [-3.21514020e-03]\n",
      " [-3.22646135e-03]\n",
      " [-3.23778205e-03]\n",
      " [-3.24910320e-03]\n",
      " [-3.26042436e-03]\n",
      " [-3.27174482e-03]\n",
      " [-3.28306574e-03]\n",
      " [-3.29438667e-03]\n",
      " [-3.30570759e-03]\n",
      " [-3.31702828e-03]\n",
      " [-3.32834967e-03]\n",
      " [-3.33967060e-03]\n",
      " [-3.35099129e-03]\n",
      " [-3.36231245e-03]\n",
      " [-3.37363291e-03]\n",
      " [-3.38495383e-03]\n",
      " [-3.39627499e-03]\n",
      " [-3.40759614e-03]\n",
      " [-3.41891730e-03]\n",
      " [-3.43023823e-03]\n",
      " [-3.44155869e-03]\n",
      " [-3.45287938e-03]\n",
      " [-3.46420030e-03]\n",
      " [-3.47552099e-03]\n",
      " [-3.48684262e-03]\n",
      " [-3.49816354e-03]\n",
      " [-3.50948400e-03]\n",
      " [-3.52080469e-03]\n",
      " [-3.53212608e-03]\n",
      " [-3.54344700e-03]\n",
      " [-3.55476793e-03]\n",
      " [-3.56608909e-03]\n",
      " [-3.57740978e-03]\n",
      " [-3.58873047e-03]\n",
      " [-3.60005163e-03]\n",
      " [-3.61137255e-03]\n",
      " [-3.62269348e-03]\n",
      " [-3.63401417e-03]\n",
      " [-3.64533509e-03]\n",
      " [-3.65665648e-03]\n",
      " [-3.66797694e-03]\n",
      " [-3.67929810e-03]\n",
      " [-3.69061856e-03]\n",
      " [-3.70193948e-03]\n",
      " [-3.71326064e-03]\n",
      " [-3.72458110e-03]\n",
      " [-3.73590249e-03]\n",
      " [-3.74722341e-03]\n",
      " [-3.75854434e-03]\n",
      " [-3.76986503e-03]\n",
      " [-3.78118595e-03]\n",
      " [-3.79250711e-03]\n",
      " [-3.80382757e-03]\n",
      " [-3.81514896e-03]\n",
      " [-3.82646965e-03]\n",
      " [-3.83779081e-03]\n",
      " [-3.84911196e-03]\n",
      " [-3.86043219e-03]\n",
      " [-3.87175288e-03]\n",
      " [-3.88307404e-03]\n",
      " [-3.89439519e-03]\n",
      " [-3.90571612e-03]\n",
      " [-3.91703704e-03]\n",
      " [-3.92835913e-03]\n",
      " [-3.93967889e-03]\n",
      " [-3.95099958e-03]\n",
      " [-3.96232074e-03]\n",
      " [-3.97364190e-03]\n",
      " [-3.98496212e-03]\n",
      " [-3.99628421e-03]\n",
      " [-4.00760444e-03]\n",
      " [-4.01892606e-03]\n",
      " [-4.03024629e-03]\n",
      " [-4.04156698e-03]\n",
      " [-4.05288721e-03]\n",
      " [-4.06420929e-03]\n",
      " [-4.07552999e-03]\n",
      " [-4.08685114e-03]\n",
      " [-4.09817137e-03]\n",
      " [-4.10949299e-03]\n",
      " [-4.12081415e-03]\n",
      " [-4.13213437e-03]\n",
      " [-4.14345507e-03]\n",
      " [-4.15477622e-03]\n",
      " [-4.16609831e-03]\n",
      " [-4.17741854e-03]\n",
      " [-4.18873969e-03]\n",
      " [-4.20006085e-03]\n",
      " [-4.21138108e-03]\n",
      " [-4.22270130e-03]\n",
      " [-4.23402293e-03]\n",
      " [-4.24534362e-03]\n",
      " [-4.25666524e-03]\n",
      " [-4.26798500e-03]\n",
      " [-4.27930616e-03]\n",
      " [-4.29062732e-03]\n",
      " [-4.30194801e-03]\n",
      " [-4.31326916e-03]\n",
      " [-4.32459032e-03]\n",
      " [-4.33591101e-03]\n",
      " [-4.34723264e-03]\n",
      " [-4.35855379e-03]\n",
      " [-4.36987355e-03]\n",
      " [-4.38119471e-03]\n",
      " [-4.39251540e-03]\n",
      " [-4.40383656e-03]\n",
      " [-4.41515772e-03]\n",
      " [-4.42647887e-03]\n",
      " [-4.43779957e-03]\n",
      " [-4.44912026e-03]\n",
      " [-4.46044141e-03]\n",
      " [-4.47176211e-03]\n",
      " [-4.48308233e-03]\n",
      " [-4.49440349e-03]\n",
      " [-4.50572465e-03]\n",
      " [-4.51704627e-03]\n",
      " [-4.52836696e-03]\n",
      " [-4.53968672e-03]\n",
      " [-4.55100834e-03]\n",
      " [-4.56232904e-03]\n",
      " [-4.57364973e-03]\n",
      " [-4.58497135e-03]\n",
      " [-4.59629204e-03]\n",
      " [-4.60761273e-03]\n",
      " [-4.61893342e-03]\n",
      " [-4.63025458e-03]\n",
      " [-4.64157667e-03]\n",
      " [-4.65289643e-03]\n",
      " [-4.66421712e-03]\n",
      " [-4.67553874e-03]\n",
      " [-4.68685990e-03]\n",
      " [-4.69818059e-03]\n",
      " [-4.70950129e-03]\n",
      " [-4.72082198e-03]\n",
      " [-4.73214313e-03]\n",
      " [-4.74346429e-03]\n",
      " [-4.75478405e-03]\n",
      " [-4.76610614e-03]\n",
      " [-4.77742683e-03]\n",
      " [-4.78874706e-03]\n",
      " [-4.80006821e-03]\n",
      " [-4.81139030e-03]\n",
      " [-4.82271053e-03]\n",
      " [-4.83403075e-03]\n",
      " [-4.84535238e-03]\n",
      " [-4.85667353e-03]\n",
      " [-4.86799376e-03]\n",
      " [-4.87931445e-03]\n",
      " [-4.89063514e-03]\n",
      " [-4.90195723e-03]\n",
      " [-4.91327746e-03]\n",
      " [-4.92459862e-03]\n",
      " [-4.93591931e-03]\n",
      " [-4.94724046e-03]\n",
      " [-4.95856069e-03]\n",
      " [-4.96988231e-03]\n",
      " [-4.98120394e-03]\n",
      " [-4.99252416e-03]\n",
      " [-5.00384532e-03]\n",
      " [-5.01516508e-03]\n",
      " [-5.02648717e-03]\n",
      " [-5.03780786e-03]\n",
      " [-5.04912902e-03]\n",
      " [-5.06045017e-03]\n",
      " [-5.07177087e-03]\n",
      " [-5.08309156e-03]\n",
      " [-5.09441271e-03]\n",
      " [-5.10573387e-03]\n",
      " [-5.11705456e-03]\n",
      " [-5.12837619e-03]\n",
      " [-5.13969595e-03]\n",
      " [-5.15101757e-03]\n",
      " [-5.16233779e-03]\n",
      " [-5.17365802e-03]\n",
      " [-5.18498011e-03]\n",
      " [-5.19630034e-03]\n",
      " [-5.20762196e-03]\n",
      " [-5.21894265e-03]\n",
      " [-5.23026334e-03]\n",
      " [-5.24158403e-03]\n",
      " [-5.25290519e-03]\n",
      " [-5.26422635e-03]\n",
      " [-5.27554704e-03]\n",
      " [-5.28686820e-03]\n",
      " [-5.29818982e-03]\n",
      " [-5.30950958e-03]\n",
      " [-5.32083074e-03]\n",
      " [-5.33215236e-03]\n",
      " [-5.34347259e-03]\n",
      " [-5.35479467e-03]\n",
      " [-5.36611490e-03]\n",
      " [-5.37743652e-03]\n",
      " [-5.38875582e-03]\n",
      " [-5.40007744e-03]\n",
      " [-5.41139813e-03]\n",
      " [-5.42271929e-03]\n",
      " [-5.43404045e-03]\n",
      " [-5.44536114e-03]\n",
      " [-5.45668090e-03]\n",
      " [-5.46800345e-03]\n",
      " [-5.47932368e-03]\n",
      " [-5.49064483e-03]\n",
      " [-5.50196506e-03]\n",
      " [-5.51328668e-03]\n",
      " [-5.52460738e-03]\n",
      " [-5.53592807e-03]\n",
      " [-5.54724969e-03]\n",
      " [-5.55856992e-03]\n",
      " [-5.56989061e-03]\n",
      " [-5.58121223e-03]\n",
      " [-5.59253292e-03]\n",
      " [-5.60385361e-03]\n",
      " [-5.61517524e-03]\n",
      " [-5.62649500e-03]\n",
      " [-5.63781708e-03]\n",
      " [-5.64913731e-03]\n",
      " [-5.66045800e-03]\n",
      " [-5.67177869e-03]\n",
      " [-5.68310032e-03]\n",
      " [-5.69442054e-03]\n",
      " [-5.70574217e-03]\n",
      " [-5.71706332e-03]\n",
      " [-5.72838448e-03]\n",
      " [-5.73970471e-03]\n",
      " [-5.75102586e-03]\n",
      " [-5.76234609e-03]\n",
      " [-5.77366818e-03]\n",
      " [-5.78498840e-03]\n",
      " [-5.79630909e-03]\n",
      " [-5.80762979e-03]\n",
      " [-5.81895141e-03]\n",
      " [-5.83027164e-03]\n",
      " [-5.84159372e-03]\n",
      " [-5.85291395e-03]\n",
      " [-5.86423418e-03]\n",
      " [-5.87555626e-03]\n",
      " [-5.88687696e-03]\n",
      " [-5.89819672e-03]\n",
      " [-5.90951880e-03]\n",
      " [-5.92083996e-03]\n",
      " [-5.93215972e-03]\n",
      " [-5.94348041e-03]\n",
      " [-5.95480204e-03]\n",
      " [-5.96612273e-03]\n",
      " [-5.97744389e-03]\n",
      " [-5.98876504e-03]\n",
      " [-6.00008620e-03]\n",
      " [-6.01140689e-03]\n",
      " [-6.02272712e-03]\n",
      " [-6.03404921e-03]\n",
      " [-6.04537036e-03]\n",
      " [-6.05669105e-03]\n",
      " [-6.06801081e-03]\n",
      " [-6.07933244e-03]\n",
      " [-6.09065359e-03]\n",
      " [-6.10197382e-03]\n",
      " [-6.11329405e-03]\n",
      " [-6.12461567e-03]\n",
      " [-6.13593729e-03]\n",
      " [-6.14725752e-03]\n",
      " [-6.15857774e-03]\n",
      " [-6.16989983e-03]\n",
      " [-6.18122099e-03]\n",
      " [-6.19254075e-03]\n",
      " [-6.20386191e-03]\n",
      " [-6.21518306e-03]\n",
      " [-6.22650469e-03]\n",
      " [-6.23782584e-03]\n",
      " [-6.24914514e-03]\n",
      " [-6.26046676e-03]\n",
      " [-6.27178792e-03]\n",
      " [-6.28310908e-03]\n",
      " [-6.29442930e-03]\n",
      " [-6.30575093e-03]\n",
      " [-6.31707162e-03]\n",
      " [-6.32839231e-03]\n",
      " [-6.33971347e-03]\n",
      " [-6.35103509e-03]\n",
      " [-6.36235531e-03]\n",
      " [-6.37367601e-03]\n",
      " [-6.38499763e-03]\n",
      " [-6.39631739e-03]\n",
      " [-6.40763808e-03]\n",
      " [-6.41895924e-03]\n",
      " [-6.43028039e-03]\n",
      " [-6.44160295e-03]\n",
      " [-6.45292271e-03]\n",
      " [-6.46424340e-03]\n",
      " [-6.47556409e-03]\n",
      " [-6.48688572e-03]\n",
      " [-6.49820641e-03]\n",
      " [-6.50952663e-03]\n",
      " [-6.52084872e-03]\n",
      " [-6.53216895e-03]\n",
      " [-6.54348964e-03]\n",
      " [-6.55480986e-03]\n",
      " [-6.56613149e-03]\n",
      " [-6.57745125e-03]\n",
      " [-6.58877334e-03]\n",
      " [-6.60009403e-03]\n",
      " [-6.61141519e-03]\n",
      " [-6.62273681e-03]\n",
      " [-6.63405657e-03]\n",
      " [-6.64537819e-03]\n",
      " [-6.65669935e-03]\n",
      " [-6.66802051e-03]\n",
      " [-6.67934120e-03]\n",
      " [-6.69066142e-03]\n",
      " [-6.70198258e-03]\n",
      " [-6.71330327e-03]\n",
      " [-6.72462489e-03]\n",
      " [-6.73594559e-03]\n",
      " [-6.74726581e-03]\n",
      " [-6.75858650e-03]\n",
      " [-6.76990766e-03]\n",
      " [-6.78122928e-03]\n",
      " [-6.79254998e-03]\n",
      " [-6.80387067e-03]\n",
      " [-6.81519229e-03]\n",
      " [-6.82651298e-03]\n",
      " [-6.83783460e-03]\n",
      " [-6.84915483e-03]\n",
      " [-6.86047645e-03]\n",
      " [-6.87179575e-03]\n",
      " [-6.88311737e-03]\n",
      " [-6.89443806e-03]\n",
      " [-6.90575875e-03]\n",
      " [-6.91708038e-03]\n",
      " [-6.92840060e-03]\n",
      " [-6.93972176e-03]\n",
      " [-6.95104199e-03]\n",
      " [-6.96236361e-03]\n",
      " [-6.97368523e-03]\n",
      " [-6.98500639e-03]\n",
      " [-6.99632708e-03]\n",
      " [-7.00764824e-03]\n",
      " [-7.01896800e-03]\n",
      " [-7.03028962e-03]\n",
      " [-7.04160938e-03]\n",
      " [-7.05293054e-03]\n",
      " [-7.06425216e-03]\n",
      " [-7.07557192e-03]\n",
      " [-7.08689401e-03]\n",
      " [-7.09821424e-03]\n",
      " [-7.10953586e-03]\n",
      " [-7.12085608e-03]\n",
      " [-7.13217817e-03]\n",
      " [-7.14349886e-03]\n",
      " [-7.15481956e-03]\n",
      " [-7.16614118e-03]\n",
      " [-7.17746094e-03]\n",
      " [-7.18878163e-03]\n",
      " [-7.20010325e-03]\n",
      " [-7.21142394e-03]\n",
      " [-7.22274510e-03]\n",
      " [-7.23406533e-03]\n",
      " [-7.24538695e-03]\n",
      " [-7.25670671e-03]\n",
      " [-7.26802833e-03]\n",
      " [-7.27934856e-03]\n",
      " [-7.29067018e-03]\n",
      " [-7.30199181e-03]\n",
      " [-7.31331296e-03]\n",
      " [-7.32463459e-03]\n",
      " [-7.33595388e-03]\n",
      " [-7.34727504e-03]\n",
      " [-7.35859619e-03]\n",
      " [-7.36991689e-03]\n",
      " [-7.38123711e-03]\n",
      " [-7.39255734e-03]\n",
      " [-7.40387896e-03]\n",
      " [-7.41520012e-03]\n",
      " [-7.42652128e-03]\n",
      " [-7.43784197e-03]\n",
      " [-7.44916219e-03]\n",
      " [-7.46048288e-03]\n",
      " [-7.47180497e-03]\n",
      " [-7.48312660e-03]\n",
      " [-7.49444682e-03]\n",
      " [-7.50576798e-03]\n",
      " [-7.51708867e-03]\n",
      " [-7.52841029e-03]\n",
      " [-7.53973005e-03]\n",
      " [-7.55105121e-03]\n",
      " [-7.56237190e-03]\n",
      " [-7.57369399e-03]\n",
      " [-7.58501422e-03]\n",
      " [-7.59633537e-03]\n",
      " [-7.60765513e-03]\n",
      " [-7.61897629e-03]\n",
      " [-7.63029791e-03]\n",
      " [-7.64162000e-03]\n",
      " [-7.65293930e-03]\n",
      " [-7.66426092e-03]\n",
      " [-7.67558161e-03]\n",
      " [-7.68690277e-03]\n",
      " [-7.69822393e-03]\n",
      " [-7.70954462e-03]\n",
      " [-7.72086438e-03]\n",
      " [-7.73218600e-03]\n",
      " [-7.74350576e-03]\n",
      " [-7.75482832e-03]\n",
      " [-7.76614808e-03]\n",
      " [-7.77746877e-03]\n",
      " [-7.78879039e-03]\n",
      " [-7.80011294e-03]\n",
      " [-7.81143224e-03]\n",
      " [-7.82275386e-03]\n",
      " [-7.83407409e-03]\n",
      " [-7.84539431e-03]\n",
      " [-7.85671826e-03]\n",
      " [-7.86803756e-03]\n",
      " [-7.87935778e-03]\n",
      " [-7.89067894e-03]\n",
      " [-7.90199917e-03]\n",
      " [-7.91331939e-03]\n",
      " [-7.92464148e-03]\n",
      " [-7.93596264e-03]\n",
      " [-7.94728380e-03]\n",
      " [-7.95860495e-03]\n",
      " [-7.96992425e-03]\n",
      " [-7.98124820e-03]\n",
      " [-7.99256843e-03]\n",
      " [-8.00388772e-03]\n",
      " [-8.01520888e-03]\n",
      " [-8.02652910e-03]\n",
      " [-8.03785212e-03]\n",
      " [-8.04917235e-03]\n",
      " [-8.06049258e-03]\n",
      " [-8.07181373e-03]\n",
      " [-8.08313396e-03]\n",
      " [-8.09445605e-03]\n",
      " [-8.10577441e-03]\n",
      " [-8.11709650e-03]\n",
      " [-8.12841859e-03]\n",
      " [-8.13973974e-03]\n",
      " [-8.15105997e-03]\n",
      " [-8.16238113e-03]\n",
      " [-8.17370228e-03]\n",
      " [-8.18502251e-03]\n",
      " [-8.19634274e-03]\n",
      " [-8.20766389e-03]\n",
      " [-8.21898598e-03]\n",
      " [-8.23030714e-03]\n",
      " [-8.24162830e-03]\n",
      " [-8.25294666e-03]\n",
      " [-8.26426875e-03]\n",
      " [-8.27558991e-03]\n",
      " [-8.28691013e-03]\n",
      " [-8.29823408e-03]\n",
      " [-8.30955245e-03]\n",
      " [-8.32087640e-03]\n",
      " [-8.33219662e-03]\n",
      " [-8.34351499e-03]\n",
      " [-8.35483707e-03]\n",
      " [-8.36615637e-03]\n",
      " [-8.37747939e-03]\n",
      " [-8.38879868e-03]\n",
      " [-8.40012170e-03]\n",
      " [-8.41144193e-03]\n",
      " [-8.42276216e-03]\n",
      " [-8.43408238e-03]\n",
      " [-8.44540261e-03]\n",
      " [-8.45672376e-03]\n",
      " [-8.46804585e-03]\n",
      " [-8.47936701e-03]\n",
      " [-8.49068724e-03]\n",
      " [-8.50200839e-03]\n",
      " [-8.51333048e-03]\n",
      " [-8.52465164e-03]\n",
      " [-8.53597000e-03]\n",
      " [-8.54729116e-03]\n",
      " [-8.55861232e-03]\n",
      " [-8.56993347e-03]\n",
      " [-8.58125463e-03]\n",
      " [-8.59257579e-03]\n",
      " [-8.60389601e-03]\n",
      " [-8.61521717e-03]\n",
      " [-8.62653833e-03]\n",
      " [-8.63785949e-03]\n",
      " [-8.64918064e-03]\n",
      " [-8.66050087e-03]\n",
      " [-8.67182203e-03]\n",
      " [-8.68314411e-03]\n",
      " [-8.69446527e-03]\n",
      " [-8.70578550e-03]\n",
      " [-8.71710759e-03]\n",
      " [-8.72842502e-03]\n",
      " [-8.73974711e-03]\n",
      " [-8.75106826e-03]\n",
      " [-8.76238942e-03]\n",
      " [-8.77370965e-03]\n",
      " [-8.78503080e-03]\n",
      " [-8.79635196e-03]\n",
      " [-8.80767312e-03]\n",
      " [-8.81899334e-03]\n",
      " [-8.83031543e-03]\n",
      " [-8.84163659e-03]\n",
      " [-8.85295775e-03]\n",
      " [-8.86427891e-03]\n",
      " [-8.87559913e-03]\n",
      " [-8.88691936e-03]\n",
      " [-8.89824051e-03]\n",
      " [-8.90955981e-03]\n",
      " [-8.92088283e-03]\n",
      " [-8.93220305e-03]\n",
      " [-8.94352421e-03]\n",
      " [-8.95484537e-03]\n",
      " [-8.96616466e-03]\n",
      " [-8.97748861e-03]\n",
      " [-8.98880698e-03]\n",
      " [-9.00012814e-03]\n",
      " [-9.01144929e-03]\n",
      " [-9.02277138e-03]\n",
      " [-9.03409254e-03]\n",
      " [-9.04541276e-03]\n",
      " [-9.05673392e-03]\n",
      " [-9.06805415e-03]\n",
      " [-9.07937344e-03]\n",
      " [-9.09069553e-03]\n",
      " [-9.10201669e-03]\n",
      " [-9.11333784e-03]\n",
      " [-9.12465807e-03]\n",
      " [-9.13597923e-03]\n",
      " [-9.14729945e-03]\n",
      " [-9.15861968e-03]\n",
      " [-9.16994270e-03]\n",
      " [-9.18126386e-03]\n",
      " [-9.19258408e-03]\n",
      " [-9.20390524e-03]\n",
      " [-9.21522547e-03]\n",
      " [-9.22654849e-03]\n",
      " [-9.23786685e-03]\n",
      " [-9.24918801e-03]\n",
      " [-9.26050916e-03]\n",
      " [-9.27183218e-03]\n",
      " [-9.28315334e-03]\n",
      " [-9.29447357e-03]\n",
      " [-9.30579286e-03]\n",
      " [-9.31711402e-03]\n",
      " [-9.32843424e-03]\n",
      " [-9.33975540e-03]\n",
      " [-9.35107749e-03]\n",
      " [-9.36239772e-03]\n",
      " [-9.37371980e-03]\n",
      " [-9.38503910e-03]\n",
      " [-9.39636119e-03]\n",
      " [-9.40768141e-03]\n",
      " [-9.41900257e-03]\n",
      " [-9.43032373e-03]\n",
      " [-9.44164395e-03]\n",
      " [-9.45296511e-03]\n",
      " [-9.46428627e-03]\n",
      " [-9.47560836e-03]\n",
      " [-9.48692858e-03]\n",
      " [-9.49824788e-03]\n",
      " [-9.50956810e-03]\n",
      " [-9.52089205e-03]\n",
      " [-9.53221228e-03]\n",
      " [-9.54353251e-03]\n",
      " [-9.55485366e-03]\n",
      " [-9.56617482e-03]\n",
      " [-9.57749411e-03]\n",
      " [-9.58881620e-03]\n",
      " [-9.60013643e-03]\n",
      " [-9.61145852e-03]\n",
      " [-9.62278061e-03]\n",
      " [-9.63409990e-03]\n",
      " [-9.64542106e-03]\n",
      " [-9.65674128e-03]\n",
      " [-9.66806151e-03]\n",
      " [-9.67938360e-03]\n",
      " [-9.69070476e-03]\n",
      " [-9.70202684e-03]\n",
      " [-9.71334707e-03]\n",
      " [-9.72466730e-03]\n",
      " [-9.73598752e-03]\n",
      " [-9.74730868e-03]\n",
      " [-9.75862890e-03]\n",
      " [-9.76995192e-03]\n",
      " [-9.78127029e-03]\n",
      " [-9.79259238e-03]\n",
      " [-9.80391447e-03]\n",
      " [-9.81523469e-03]\n",
      " [-9.82655492e-03]\n",
      " [-9.83787607e-03]\n",
      " [-9.84919723e-03]\n",
      " [-9.86051746e-03]\n",
      " [-9.87183861e-03]\n",
      " [-9.88316163e-03]\n",
      " [-9.89448093e-03]\n",
      " [-9.90580115e-03]\n",
      " [-9.91712138e-03]\n",
      " [-9.92844440e-03]\n",
      " [-9.93976463e-03]\n",
      " [-9.95108671e-03]\n",
      " [-9.96240787e-03]\n",
      " [-9.97372903e-03]\n",
      " [-9.98504832e-03]\n",
      " [-9.99637134e-03]\n",
      " [-1.00076906e-02]\n",
      " [-1.00190109e-02]\n",
      " [-1.00303302e-02]\n",
      " [-1.00416522e-02]\n",
      " [-1.00529743e-02]\n",
      " [-1.00642955e-02]\n",
      " [-1.00756157e-02]\n",
      " [-1.00869369e-02]\n",
      " [-1.00982580e-02]\n",
      " [-1.01095792e-02]\n",
      " [-1.01209003e-02]\n",
      " [-1.01322206e-02]\n",
      " [-1.01435417e-02]\n",
      " [-1.01548620e-02]\n",
      " [-1.01661831e-02]\n",
      " [-1.01775033e-02]\n",
      " [-1.01888254e-02]\n",
      " [-1.02001457e-02]\n",
      " [-1.02114677e-02]\n",
      " [-1.02227880e-02]\n",
      " [-1.02341091e-02]\n",
      " [-1.02454294e-02]\n",
      " [-1.02567524e-02]\n",
      " [-1.02680717e-02]\n",
      " [-1.02793919e-02]\n",
      " [-1.02907130e-02]\n",
      " [-1.03020351e-02]\n",
      " [-1.03133563e-02]\n",
      " [-1.03246756e-02]\n",
      " [-1.03359958e-02]\n",
      " [-1.03473160e-02]\n",
      " [-1.03586391e-02]\n",
      " [-1.03699602e-02]\n",
      " [-1.03812814e-02]\n",
      " [-1.03926007e-02]\n",
      " [-1.04039228e-02]\n",
      " [-1.04152439e-02]\n",
      " [-1.04265651e-02]\n",
      " [-1.04378853e-02]\n",
      " [-1.04492074e-02]\n",
      " [-1.04605267e-02]\n",
      " [-1.04718478e-02]\n",
      " [-1.04831681e-02]\n",
      " [-1.04944883e-02]\n",
      " [-1.05058104e-02]\n",
      " [-1.05171315e-02]\n",
      " [-1.05284527e-02]\n",
      " [-1.05397720e-02]\n",
      " [-1.05510941e-02]\n",
      " [-1.05624143e-02]\n",
      " [-1.05737364e-02]\n",
      " [-1.05850585e-02]\n",
      " [-1.05963796e-02]\n",
      " [-1.06077008e-02]\n",
      " [-1.06190192e-02]\n",
      " [-1.06303412e-02]\n",
      " [-1.06416615e-02]\n",
      " [-1.06529836e-02]\n",
      " [-1.06643047e-02]\n",
      " [-1.06756240e-02]\n",
      " [-1.06869452e-02]\n",
      " [-1.06982654e-02]\n",
      " [-1.07095893e-02]\n",
      " [-1.07209077e-02]\n",
      " [-1.07322298e-02]\n",
      " [-1.07435491e-02]\n",
      " [-1.07548730e-02]\n",
      " [-1.07661923e-02]\n",
      " [-1.07775116e-02]\n",
      " [-1.07888337e-02]\n",
      " [-1.08001549e-02]\n",
      " [-1.08114760e-02]\n",
      " [-1.08227963e-02]\n",
      " [-1.08341156e-02]\n",
      " [-1.08454386e-02]\n",
      " [-1.08567579e-02]\n",
      " [-1.08680809e-02]\n",
      " [-1.08794002e-02]\n",
      " [-1.08907223e-02]\n",
      " [-1.09020444e-02]\n",
      " [-1.09133618e-02]\n",
      " [-1.09246839e-02]\n",
      " [-1.09360069e-02]\n",
      " [-1.09473262e-02]\n",
      " [-1.09586474e-02]\n",
      " [-1.09699667e-02]\n",
      " [-1.09812897e-02]\n",
      " [-1.09926099e-02]\n",
      " [-1.10039301e-02]\n",
      " [-1.10152531e-02]\n",
      " [-1.10265734e-02]\n",
      " [-1.10378927e-02]\n",
      " [-1.10492148e-02]\n",
      " [-1.10605350e-02]\n",
      " [-1.10718561e-02]\n",
      " [-1.10831782e-02]\n",
      " [-1.10944994e-02]\n",
      " [-1.11058205e-02]\n",
      " [-1.11171398e-02]\n",
      " [-1.11284619e-02]\n",
      " [-1.11397812e-02]\n",
      " [-1.11511033e-02]\n",
      " [-1.11624245e-02]\n",
      " [-1.11737447e-02]\n",
      " [-1.11850658e-02]\n",
      " [-1.11963851e-02]\n",
      " [-1.12077072e-02]\n",
      " [-1.12190275e-02]\n",
      " [-1.12303505e-02]\n",
      " [-1.12416707e-02]\n",
      " [-1.12529900e-02]\n",
      " [-1.12643130e-02]\n",
      " [-1.12756342e-02]\n",
      " [-1.12869553e-02]\n",
      " [-1.12982746e-02]\n",
      " [-1.13095958e-02]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m model.finalize()\n\u001b[32m     20\u001b[39m output = model.train(X, y, epochs=\u001b[32m2\u001b[39m, print_every=\u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "\n",
    "X, y = sine_data()\n",
    "\n",
    "model = Model()\n",
    "model.add(Layer_Dense(1, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 1))\n",
    "model.add(Activation_Linear())\n",
    "\n",
    "model.set(\n",
    "    loss=Loss_MeanSquaredError(),\n",
    "    optimizer=Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    ")\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "output = model.train(X, y, epochs=2, print_every=100)\n",
    "output[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Model Class Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self, *, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def finalize(self):\n",
    "        # Create the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count layers\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate through and connect layers\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "    def forward(self, X):\n",
    "        # Perform forward pass\n",
    "        self.input_layer.forward(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output)\n",
    "            \n",
    "        return layer.output\n",
    "    \n",
    "    def train(self, X, y, *, epochs=1, print_every=1):\n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Perform forward pass\n",
    "            output = self.forward(X)\n",
    "            print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the per sample loss\n",
    "        samples_losses = self.forward(output, y)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = np.mean(samples_losses)\n",
    "\n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        # Init return value to 0\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "        \n",
    "            # L1 weights\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "\n",
    "            # L1 biases\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return samples_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs): \n",
    "        return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Add predictions method to Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):  # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    \n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate the output based on inputs.\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Copy before we modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Set to 0 if value is <=0\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Add Accuracy classes for regression and classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        accuracy = np.mean(comparisons)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "        \n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "            \n",
    "    def compare(self, predictions, y):\n",
    "        return np.abs(predictions - y) < self.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    def finalize(self):\n",
    "        # Create the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count layers\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate through and connect layers\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "        # Update loss with trainable layers\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        # Perform forward pass\n",
    "        self.input_layer.forward(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output)\n",
    "            \n",
    "        return layer.output\n",
    "    \n",
    "    def train(self, X, y, *, epochs=1, print_every=1):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Perform forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            data_loss, regularization_loss = self.loss.calculate(output, y)\n",
    "            loss = data_loss + regularization_loss\n",
    "            \n",
    "            # Get predictions and compute accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "            \n",
    "            # Perform backward pass\n",
    "            self.backward(output, y)\n",
    "            \n",
    "            # Run Optimizer\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "            \n",
    "            # Print\n",
    "            if not epoch % print_every:\n",
    "                print(f\"Epoch: {epoch}, Acc: {accuracy:.3f}, Loss: {loss:.6f}, Data Loss: {data_loss:.6f}, Reg Loss: {regularization_loss:.6f}, LR: {self.optimizer.current_learning_rate:.6f}\")\n",
    "\n",
    "            \n",
    "    def backward(self, output, y):\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward methods for all layers in reverse\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Acc: 0.017, Loss: 0.049211, Data Loss: 0.049211, Reg Loss: 0.000000, LR: 0.004550\n",
      "Epoch: 200, Acc: 0.420, Loss: 0.000787, Data Loss: 0.000787, Reg Loss: 0.000000, LR: 0.004170\n",
      "Epoch: 300, Acc: 0.787, Loss: 0.000066, Data Loss: 0.000066, Reg Loss: 0.000000, LR: 0.003849\n",
      "Epoch: 400, Acc: 0.885, Loss: 0.000020, Data Loss: 0.000020, Reg Loss: 0.000000, LR: 0.003574\n",
      "Epoch: 500, Acc: 0.053, Loss: 0.000121, Data Loss: 0.000121, Reg Loss: 0.000000, LR: 0.003336\n",
      "Epoch: 600, Acc: 0.913, Loss: 0.000006, Data Loss: 0.000006, Reg Loss: 0.000000, LR: 0.003127\n",
      "Epoch: 700, Acc: 0.922, Loss: 0.000004, Data Loss: 0.000004, Reg Loss: 0.000000, LR: 0.002943\n",
      "Epoch: 800, Acc: 0.040, Loss: 0.000305, Data Loss: 0.000305, Reg Loss: 0.000000, LR: 0.002779\n",
      "Epoch: 900, Acc: 0.932, Loss: 0.000003, Data Loss: 0.000003, Reg Loss: 0.000000, LR: 0.002633\n",
      "Epoch: 1000, Acc: 0.936, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.002501\n",
      "Epoch: 1100, Acc: 0.685, Loss: 0.000007, Data Loss: 0.000007, Reg Loss: 0.000000, LR: 0.002382\n",
      "Epoch: 1200, Acc: 0.938, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.002274\n",
      "Epoch: 1300, Acc: 0.940, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.002175\n",
      "Epoch: 1400, Acc: 0.933, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.002084\n",
      "Epoch: 1500, Acc: 0.943, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.002001\n",
      "Epoch: 1600, Acc: 0.159, Loss: 0.000037, Data Loss: 0.000037, Reg Loss: 0.000000, LR: 0.001924\n",
      "Epoch: 1700, Acc: 0.940, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001853\n",
      "Epoch: 1800, Acc: 0.943, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001786\n",
      "Epoch: 1900, Acc: 0.110, Loss: 0.000114, Data Loss: 0.000114, Reg Loss: 0.000000, LR: 0.001725\n",
      "Epoch: 2000, Acc: 0.947, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001667\n",
      "Epoch: 2100, Acc: 0.941, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001613\n",
      "Epoch: 2200, Acc: 0.194, Loss: 0.000027, Data Loss: 0.000027, Reg Loss: 0.000000, LR: 0.001563\n",
      "Epoch: 2300, Acc: 0.943, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001516\n",
      "Epoch: 2400, Acc: 0.946, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001471\n",
      "Epoch: 2500, Acc: 0.860, Loss: 0.000004, Data Loss: 0.000004, Reg Loss: 0.000000, LR: 0.001429\n",
      "Epoch: 2600, Acc: 0.947, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001389\n",
      "Epoch: 2700, Acc: 0.951, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001352\n",
      "Epoch: 2800, Acc: 0.949, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001316\n",
      "Epoch: 2900, Acc: 0.951, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001282\n",
      "Epoch: 3000, Acc: 0.951, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001250\n",
      "Epoch: 3100, Acc: 0.937, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001220\n",
      "Epoch: 3200, Acc: 0.948, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001191\n",
      "Epoch: 3300, Acc: 0.950, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001163\n",
      "Epoch: 3400, Acc: 0.949, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001137\n",
      "Epoch: 3500, Acc: 0.953, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001111\n",
      "Epoch: 3600, Acc: 0.953, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001087\n",
      "Epoch: 3700, Acc: 0.954, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001064\n",
      "Epoch: 3800, Acc: 0.953, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001042\n",
      "Epoch: 3900, Acc: 0.949, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001021\n",
      "Epoch: 4000, Acc: 0.953, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.001000\n",
      "Epoch: 4100, Acc: 0.954, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000981\n",
      "Epoch: 4200, Acc: 0.957, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000962\n",
      "Epoch: 4300, Acc: 0.134, Loss: 0.000025, Data Loss: 0.000025, Reg Loss: 0.000000, LR: 0.000944\n",
      "Epoch: 4400, Acc: 0.954, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000926\n",
      "Epoch: 4500, Acc: 0.957, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000909\n",
      "Epoch: 4600, Acc: 0.858, Loss: 0.000005, Data Loss: 0.000005, Reg Loss: 0.000000, LR: 0.000893\n",
      "Epoch: 4700, Acc: 0.957, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000877\n",
      "Epoch: 4800, Acc: 0.958, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000862\n",
      "Epoch: 4900, Acc: 0.960, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000848\n",
      "Epoch: 5000, Acc: 0.959, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000833\n",
      "Epoch: 5100, Acc: 0.960, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000820\n",
      "Epoch: 5200, Acc: 0.961, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000807\n",
      "Epoch: 5300, Acc: 0.961, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000794\n",
      "Epoch: 5400, Acc: 0.959, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000781\n",
      "Epoch: 5500, Acc: 0.961, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000769\n",
      "Epoch: 5600, Acc: 0.959, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000758\n",
      "Epoch: 5700, Acc: 0.113, Loss: 0.000034, Data Loss: 0.000034, Reg Loss: 0.000000, LR: 0.000746\n",
      "Epoch: 5800, Acc: 0.962, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000735\n",
      "Epoch: 5900, Acc: 0.962, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000725\n",
      "Epoch: 6000, Acc: 0.972, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000714\n",
      "Epoch: 6100, Acc: 0.965, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000704\n",
      "Epoch: 6200, Acc: 0.968, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000695\n",
      "Epoch: 6300, Acc: 0.964, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000685\n",
      "Epoch: 6400, Acc: 0.964, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000676\n",
      "Epoch: 6500, Acc: 0.964, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000667\n",
      "Epoch: 6600, Acc: 0.487, Loss: 0.000009, Data Loss: 0.000009, Reg Loss: 0.000000, LR: 0.000658\n",
      "Epoch: 6700, Acc: 0.971, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000649\n",
      "Epoch: 6800, Acc: 0.968, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000641\n",
      "Epoch: 6900, Acc: 0.970, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000633\n",
      "Epoch: 7000, Acc: 0.973, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000625\n",
      "Epoch: 7100, Acc: 0.976, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000617\n",
      "Epoch: 7200, Acc: 0.967, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000610\n",
      "Epoch: 7300, Acc: 0.977, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000602\n",
      "Epoch: 7400, Acc: 0.977, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000595\n",
      "Epoch: 7500, Acc: 0.978, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000588\n",
      "Epoch: 7600, Acc: 0.825, Loss: 0.000005, Data Loss: 0.000005, Reg Loss: 0.000000, LR: 0.000581\n",
      "Epoch: 7700, Acc: 0.978, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000575\n",
      "Epoch: 7800, Acc: 0.941, Loss: 0.000004, Data Loss: 0.000004, Reg Loss: 0.000000, LR: 0.000568\n",
      "Epoch: 7900, Acc: 0.978, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000562\n",
      "Epoch: 8000, Acc: 0.961, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000556\n",
      "Epoch: 8100, Acc: 0.978, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000550\n",
      "Epoch: 8200, Acc: 0.961, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000544\n",
      "Epoch: 8300, Acc: 0.979, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000538\n",
      "Epoch: 8400, Acc: 0.972, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000532\n",
      "Epoch: 8500, Acc: 0.980, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000526\n",
      "Epoch: 8600, Acc: 0.981, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000521\n",
      "Epoch: 8700, Acc: 0.980, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000516\n",
      "Epoch: 8800, Acc: 0.980, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000510\n",
      "Epoch: 8900, Acc: 0.980, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000505\n",
      "Epoch: 9000, Acc: 0.981, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000500\n",
      "Epoch: 9100, Acc: 0.444, Loss: 0.000010, Data Loss: 0.000010, Reg Loss: 0.000000, LR: 0.000495\n",
      "Epoch: 9200, Acc: 0.982, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000490\n",
      "Epoch: 9300, Acc: 0.354, Loss: 0.000013, Data Loss: 0.000013, Reg Loss: 0.000000, LR: 0.000485\n",
      "Epoch: 9400, Acc: 0.982, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000481\n",
      "Epoch: 9500, Acc: 0.977, Loss: 0.000002, Data Loss: 0.000002, Reg Loss: 0.000000, LR: 0.000476\n",
      "Epoch: 9600, Acc: 0.983, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000472\n",
      "Epoch: 9700, Acc: 0.982, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000467\n",
      "Epoch: 9800, Acc: 0.963, Loss: 0.000003, Data Loss: 0.000003, Reg Loss: 0.000000, LR: 0.000463\n",
      "Epoch: 9900, Acc: 0.982, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000459\n",
      "Epoch: 10000, Acc: 0.982, Loss: 0.000001, Data Loss: 0.000001, Reg Loss: 0.000000, LR: 0.000455\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "\n",
    "X, y = sine_data()\n",
    "\n",
    "model = Model()\n",
    "model.add(Layer_Dense(1, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 1))\n",
    "model.add(Activation_Linear())\n",
    "\n",
    "model.set(\n",
    "    loss=Loss_MeanSquaredError(),\n",
    "    optimizer=Optimizer_Adam(learning_rate=0.005, decay=1e-3),\n",
    "    accuracy=Accuracy_Regression()\n",
    ")\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X, y, epochs=10000, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Now let's make it work for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical(Accuracy):\n",
    "    def __init__(self, *, binary=False):\n",
    "        self.binary = binary\n",
    "        \n",
    "    def init(self, y):\n",
    "        pass\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Let's add validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        # Calculate the per sample loss\n",
    "        samples_losses = self.forward(output, y)\n",
    "        \n",
    "        # Compute data loss\n",
    "        data_loss = np.mean(samples_losses)\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def regularization_loss(self):\n",
    "        # Init return value to 0\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        for layer in self.trainable_layers:\n",
    "        \n",
    "            # L1 weights\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "\n",
    "            # L1 biases\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    def finalize(self):\n",
    "        # Create the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count layers\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate through and connect layers\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "        # Update loss with trainable layers\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        # Perform forward pass\n",
    "        self.input_layer.forward(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output)\n",
    "            \n",
    "        return layer.output\n",
    "    \n",
    "    def train(self, X, y, *, epochs=1, print_every=1, validation_data=None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Perform forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            data_loss, regularization_loss = self.loss.calculate(output, y, \n",
    "                                                                 include_regularization=True)\n",
    "            loss = data_loss + regularization_loss\n",
    "            \n",
    "            # Get predictions and compute accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "            \n",
    "            # Perform backward pass\n",
    "            self.backward(output, y)\n",
    "            \n",
    "            # Run Optimizer\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "            \n",
    "            # Print\n",
    "            if not epoch % print_every: \n",
    "                print(\n",
    "                    f'epoch: {epoch}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "                    f'lr: {self.optimizer.current_learning_rate:.5f}'\n",
    "                )\n",
    "                \n",
    "        # Compute validation performance if validation data exists\n",
    "        if validation_data is not None:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            output = self.forward(X_val)\n",
    "            \n",
    "            loss = self.loss.calculate(output, y_val)\n",
    "            \n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "            \n",
    "            print(f\"Validation: acc: {accuracy:.3f}, loss: {loss:.3f}\")\n",
    "            \n",
    "    def backward(self, output, y):\n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward methods for all layers in reverse\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, acc: 0.635, loss: 0.657 (data_loss: 0.654, reg_loss: 0.003), lr: 0.00100\n",
      "epoch: 1000, acc: 0.735, loss: 0.597 (data_loss: 0.580, reg_loss: 0.017), lr: 0.00100\n",
      "epoch: 1500, acc: 0.835, loss: 0.542 (data_loss: 0.509, reg_loss: 0.034), lr: 0.00100\n",
      "epoch: 2000, acc: 0.835, loss: 0.494 (data_loss: 0.445, reg_loss: 0.049), lr: 0.00100\n",
      "epoch: 2500, acc: 0.870, loss: 0.439 (data_loss: 0.374, reg_loss: 0.065), lr: 0.00100\n",
      "epoch: 3000, acc: 0.915, loss: 0.397 (data_loss: 0.321, reg_loss: 0.076), lr: 0.00100\n",
      "epoch: 3500, acc: 0.950, loss: 0.355 (data_loss: 0.267, reg_loss: 0.088), lr: 0.00100\n",
      "epoch: 4000, acc: 0.955, loss: 0.325 (data_loss: 0.231, reg_loss: 0.093), lr: 0.00100\n",
      "epoch: 4500, acc: 0.970, loss: 0.302 (data_loss: 0.207, reg_loss: 0.095), lr: 0.00100\n",
      "epoch: 5000, acc: 0.970, loss: 0.283 (data_loss: 0.188, reg_loss: 0.095), lr: 0.00100\n",
      "epoch: 5500, acc: 0.980, loss: 0.266 (data_loss: 0.172, reg_loss: 0.093), lr: 0.00100\n",
      "epoch: 6000, acc: 0.980, loss: 0.251 (data_loss: 0.159, reg_loss: 0.091), lr: 0.00100\n",
      "epoch: 6500, acc: 0.980, loss: 0.238 (data_loss: 0.149, reg_loss: 0.089), lr: 0.00100\n",
      "epoch: 7000, acc: 0.980, loss: 0.226 (data_loss: 0.140, reg_loss: 0.086), lr: 0.00100\n",
      "epoch: 7500, acc: 0.980, loss: 0.216 (data_loss: 0.132, reg_loss: 0.084), lr: 0.00100\n",
      "epoch: 8000, acc: 0.985, loss: 0.204 (data_loss: 0.122, reg_loss: 0.082), lr: 0.00100\n",
      "epoch: 8500, acc: 0.985, loss: 0.195 (data_loss: 0.115, reg_loss: 0.080), lr: 0.00100\n",
      "epoch: 9000, acc: 0.985, loss: 0.186 (data_loss: 0.109, reg_loss: 0.078), lr: 0.00100\n",
      "epoch: 9500, acc: 0.990, loss: 0.179 (data_loss: 0.104, reg_loss: 0.075), lr: 0.00100\n",
      "epoch: 10000, acc: 0.990, loss: 0.172 (data_loss: 0.099, reg_loss: 0.073), lr: 0.00100\n",
      "Validation: acc: 0.950, loss: 0.211\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=2)\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 1))\n",
    "model.add(Activation_Sigmoid())\n",
    "\n",
    "model.set(\n",
    "    loss=Loss_BinaryCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=5e-7),\n",
    "    accuracy=Accuracy_Categorical(binary=True)    \n",
    ")\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10000, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Adapting dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        # Store the dropout probability which is 1-dropout_rate\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self, inputs, training):\n",
    "        # Save inputs\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # If we are not in Training, just return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Apply output mask\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Apply gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    def finalize(self):\n",
    "        # Create the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "        \n",
    "        # Count layers\n",
    "        layer_count = len(self.layers)\n",
    "        \n",
    "        self.trainable_layers = []\n",
    "        \n",
    "        # Iterate through and connect layers\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            elif i < layer_count-1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "                \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "                \n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "                \n",
    "        # Update loss with trainable layers\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "        \n",
    "        # If Softmax + Categorical Crossentropy use the combo function\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "                \n",
    "    def forward(self, X, training):\n",
    "        # Perform forward pass\n",
    "        self.input_layer.forward(X, training)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "            \n",
    "        return layer.output\n",
    "    \n",
    "    def train(self, X, y, *, epochs=1, print_every=1, validation_data=None):\n",
    "        self.accuracy.init(y)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Perform forward pass\n",
    "            output = self.forward(X, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            data_loss, regularization_loss = self.loss.calculate(output, y, \n",
    "                                                                 include_regularization=True)\n",
    "            loss = data_loss + regularization_loss\n",
    "            \n",
    "            # Get predictions and compute accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "            \n",
    "            # Perform backward pass\n",
    "            self.backward(output, y)\n",
    "            \n",
    "            # Run Optimizer\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "            \n",
    "            # Print\n",
    "            if not epoch % print_every: \n",
    "                print(\n",
    "                    f'epoch: {epoch}, ' +\n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss: {loss:.3f} (' +\n",
    "                    f'data_loss: {data_loss:.3f}, ' +\n",
    "                    f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "                    f'lr: {self.optimizer.current_learning_rate:.5f}'\n",
    "                )\n",
    "                \n",
    "        # Compute validation performance if validation data exists\n",
    "        if validation_data is not None:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            output = self.forward(X_val, training=False)\n",
    "            \n",
    "            loss = self.loss.calculate(output, y_val)\n",
    "            \n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "            \n",
    "            print(f\"Validation: acc: {accuracy:.3f}, loss: {loss:.3f}\")\n",
    "            \n",
    "    def backward(self, output, y):\n",
    "        # If softmax + categorical crossentropy\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.loss.backward(output, y)\n",
    "        \n",
    "        # Call backward methods for all layers in reverse\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Input:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Initialization Code\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initilalize weights and biases according to the shape given\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Save Regularization Lambdas\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output as we did on the slides\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Regularisation\n",
    "        # L1 weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate the output based on inputs.\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Copy before we modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Set to 0 if value is <=0\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "    def predictions(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)  # Count the samples\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Clip the predictions\n",
    "\n",
    "        # Get correct confidence values\n",
    "        # if labels are sparse\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "\n",
    "        # else if labels are one hot encoded\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Compute Losses\n",
    "        losses = -np.log(correct_confidences)\n",
    "        return losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs, training):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs): \n",
    "        return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def backward(self, dvalues, y_true):  # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, acc: 0.815, loss: 0.536 (data_loss: 0.459, reg_loss: 0.077), lr: 0.04878\n",
      "epoch: 1000, acc: 0.837, loss: 0.490 (data_loss: 0.423, reg_loss: 0.067), lr: 0.04762\n",
      "epoch: 1500, acc: 0.837, loss: 0.498 (data_loss: 0.432, reg_loss: 0.065), lr: 0.04651\n",
      "epoch: 2000, acc: 0.851, loss: 0.481 (data_loss: 0.420, reg_loss: 0.062), lr: 0.04546\n",
      "epoch: 2500, acc: 0.850, loss: 0.463 (data_loss: 0.397, reg_loss: 0.066), lr: 0.04445\n",
      "epoch: 3000, acc: 0.840, loss: 0.477 (data_loss: 0.420, reg_loss: 0.058), lr: 0.04348\n",
      "epoch: 3500, acc: 0.851, loss: 0.462 (data_loss: 0.404, reg_loss: 0.058), lr: 0.04256\n",
      "epoch: 4000, acc: 0.849, loss: 0.455 (data_loss: 0.399, reg_loss: 0.056), lr: 0.04167\n",
      "epoch: 4500, acc: 0.850, loss: 0.433 (data_loss: 0.380, reg_loss: 0.053), lr: 0.04082\n",
      "epoch: 5000, acc: 0.840, loss: 0.479 (data_loss: 0.427, reg_loss: 0.052), lr: 0.04000\n",
      "epoch: 5500, acc: 0.859, loss: 0.422 (data_loss: 0.371, reg_loss: 0.051), lr: 0.03922\n",
      "epoch: 6000, acc: 0.851, loss: 0.443 (data_loss: 0.393, reg_loss: 0.050), lr: 0.03846\n",
      "epoch: 6500, acc: 0.862, loss: 0.433 (data_loss: 0.383, reg_loss: 0.050), lr: 0.03774\n",
      "epoch: 7000, acc: 0.861, loss: 0.424 (data_loss: 0.374, reg_loss: 0.050), lr: 0.03704\n",
      "epoch: 7500, acc: 0.869, loss: 0.423 (data_loss: 0.373, reg_loss: 0.050), lr: 0.03636\n",
      "epoch: 8000, acc: 0.862, loss: 0.418 (data_loss: 0.369, reg_loss: 0.049), lr: 0.03572\n",
      "epoch: 8500, acc: 0.844, loss: 0.486 (data_loss: 0.421, reg_loss: 0.065), lr: 0.03509\n",
      "epoch: 9000, acc: 0.805, loss: 0.527 (data_loss: 0.460, reg_loss: 0.066), lr: 0.03448\n",
      "epoch: 9500, acc: 0.863, loss: 0.457 (data_loss: 0.404, reg_loss: 0.053), lr: 0.03390\n",
      "epoch: 10000, acc: 0.843, loss: 0.461 (data_loss: 0.413, reg_loss: 0.048), lr: 0.03333\n",
      "Validation: acc: 0.837, loss: 0.408\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model.add(Layer_Dense(2, 512, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(Layer_Dense(512, 3))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(learning_rate=0.05, decay=5e-5),\n",
    "    accuracy=Accuracy_Categorical()    \n",
    ")\n",
    "\n",
    "model.finalize()\n",
    "\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10000, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f30131-27b5-4b77-9c22-f6a48d091a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
