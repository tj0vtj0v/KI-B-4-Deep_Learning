{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5fe60d",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "Deep Learning - KI29  \n",
    "Deggendorf Institute of Technology  \n",
    "Prof. Dr. Florian Wahl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d50d5",
   "metadata": {},
   "source": [
    "## Existierende Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26003db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4251dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Initialization Code\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initilalize weights and biases according to the shape given\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Save Regularization Lambdas\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output as we did on the slides\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Regularisation\n",
    "        # L1 weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fe9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate the output based on inputs.\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Copy before we modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Set to 0 if value is <=0\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647a0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the per sample loss\n",
    "        samples_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate the mean loss and return it\n",
    "        loss = np.mean(samples_losses)\n",
    "        return loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # Init return value to 0\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "            \n",
    "        # L1 biases\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "        # L2 biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7581967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)  # Count the samples\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Clip the predictions\n",
    "\n",
    "        # Get correct confidence values\n",
    "        # if labels are sparse\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "\n",
    "        # else if labels are one hot encoded\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Compute Losses\n",
    "        losses = -np.log(correct_confidences)\n",
    "        return losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08887229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dddaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):  # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a655e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            # If layer has no momentum arrays, create them\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Compute weight updates\n",
    "            weight_updates = (\n",
    "                self.momentum * layer.weight_momentums\n",
    "                - self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Compute bias updates\n",
    "            bias_updates = (\n",
    "                self.momentum * layer.bias_momentums\n",
    "                - self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Perform update\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a391a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3848811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        )\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbceb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.001, decay=0.0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Compute momentums\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Perform momentum correction using beta 1 (add +1 to avoid zero-division error)\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        )\n",
    "        \n",
    "        # Perform cache correction\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * weight_momentums_corrected\n",
    "            / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * bias_momentums_corrected\n",
    "            / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a29edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        # Store the dropout probability which is 1-dropout_rate\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Save inputs\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Apply output mask\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Apply gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd10dc9",
   "metadata": {},
   "source": [
    "## Sigmoid Aktivierungsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24fcc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        ...\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4801bc",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7c760",
   "metadata": {},
   "source": [
    "## Binäre Kreuzentropie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9404b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        ...\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        ...\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        ...\n",
    "\n",
    "        # Calculate gradient\n",
    "        ...\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495c0b97",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e973c91e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.490, loss: 0.693, data_loss: 0.693, reg_loss: 0.000, lr: 0.00100000\n",
      "epoch: 500, acc: 0.640, loss: 0.651, data_loss: 0.648, reg_loss: 0.003, lr: 0.00099975\n",
      "epoch: 1000, acc: 0.675, loss: 0.626, data_loss: 0.618, reg_loss: 0.008, lr: 0.00099950\n",
      "epoch: 1500, acc: 0.730, loss: 0.574, data_loss: 0.557, reg_loss: 0.017, lr: 0.00099925\n",
      "epoch: 2000, acc: 0.815, loss: 0.505, data_loss: 0.469, reg_loss: 0.035, lr: 0.00099900\n",
      "epoch: 2500, acc: 0.860, loss: 0.455, data_loss: 0.405, reg_loss: 0.050, lr: 0.00099875\n",
      "epoch: 3000, acc: 0.905, loss: 0.406, data_loss: 0.343, reg_loss: 0.062, lr: 0.00099850\n",
      "epoch: 3500, acc: 0.910, loss: 0.375, data_loss: 0.303, reg_loss: 0.072, lr: 0.00099825\n",
      "epoch: 4000, acc: 0.925, loss: 0.354, data_loss: 0.277, reg_loss: 0.077, lr: 0.00099800\n",
      "epoch: 4500, acc: 0.925, loss: 0.337, data_loss: 0.260, reg_loss: 0.078, lr: 0.00099776\n",
      "epoch: 5000, acc: 0.925, loss: 0.320, data_loss: 0.242, reg_loss: 0.078, lr: 0.00099751\n",
      "epoch: 5500, acc: 0.925, loss: 0.307, data_loss: 0.229, reg_loss: 0.077, lr: 0.00099726\n",
      "epoch: 6000, acc: 0.945, loss: 0.282, data_loss: 0.204, reg_loss: 0.078, lr: 0.00099701\n",
      "epoch: 6500, acc: 0.955, loss: 0.262, data_loss: 0.183, reg_loss: 0.079, lr: 0.00099676\n",
      "epoch: 7000, acc: 0.960, loss: 0.250, data_loss: 0.171, reg_loss: 0.079, lr: 0.00099651\n",
      "epoch: 7500, acc: 0.965, loss: 0.239, data_loss: 0.162, reg_loss: 0.077, lr: 0.00099626\n",
      "epoch: 8000, acc: 0.965, loss: 0.230, data_loss: 0.155, reg_loss: 0.076, lr: 0.00099602\n",
      "epoch: 8500, acc: 0.965, loss: 0.223, data_loss: 0.149, reg_loss: 0.074, lr: 0.00099577\n",
      "epoch: 9000, acc: 0.965, loss: 0.216, data_loss: 0.144, reg_loss: 0.072, lr: 0.00099552\n",
      "epoch: 9500, acc: 0.970, loss: 0.210, data_loss: 0.140, reg_loss: 0.070, lr: 0.00099527\n",
      "epoch: 10000, acc: 0.970, loss: 0.204, data_loss: 0.136, reg_loss: 0.067, lr: 0.00099503\n",
      "validation, acc: 0.920, loss: 0.240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Sigmoid activation:\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(decay=5e-7)#\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "learning_rates = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values, multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if not epoch % 500:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate:.8f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    learning_rates.append(optimizer.current_learning_rate)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the data loss\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Part in the brackets returns a binary mask - array consisting of\n",
    "# True/False values, multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "df = pd.DataFrame([losses, accuracies, learning_rates]).T\n",
    "df = df.rename({0: \"loss\", 1: \"accuracy\", 2: \"learning_rate\"}, axis=\"columns\")\n",
    "df.plot(subplots=True, sharex=True, sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d6d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280a637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2258c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
